{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f934c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from keybert import KeyBERT\n",
    "import yake\n",
    "import string\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a613e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sds_data = pd.read_csv('../data/sds_ds_text.csv')\n",
    "#sds_non_data = pd.read_csv('../data/sds_nds_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ad4c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_name</th>\n",
       "      <th>length_episode</th>\n",
       "      <th>context_episode</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>guest_info</th>\n",
       "      <th>text_episode</th>\n",
       "      <th>episode_number</th>\n",
       "      <th>episode_date</th>\n",
       "      <th>episode_year</th>\n",
       "      <th>episode_day</th>\n",
       "      <th>host_episode</th>\n",
       "      <th>class</th>\n",
       "      <th>episode_split_text</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ruben Kogel on Self-Serve Analytics, R vs Pyt...</td>\n",
       "      <td>42</td>\n",
       "      <td>Business Data Science Database</td>\n",
       "      <td>Ruben Kogel</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Kirill: This is episode number one with ex-che...</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-09-10</td>\n",
       "      <td>2016</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>Database</td>\n",
       "      <td>This is episode number one with ex-chemical en...</td>\n",
       "      <td>Kirill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ruben Kogel on Self-Serve Analytics, R vs Pyt...</td>\n",
       "      <td>42</td>\n",
       "      <td>Business Data Science Database</td>\n",
       "      <td>Ruben Kogel</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Kirill: This is episode number one with ex-che...</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-09-10</td>\n",
       "      <td>2016</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>Database</td>\n",
       "      <td>Hey guys, welcome to the Podcast. I've got Rub...</td>\n",
       "      <td>Kirill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ruben Kogel on Self-Serve Analytics, R vs Pyt...</td>\n",
       "      <td>42</td>\n",
       "      <td>Business Data Science Database</td>\n",
       "      <td>Ruben Kogel</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Kirill: This is episode number one with ex-che...</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-09-10</td>\n",
       "      <td>2016</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>Database</td>\n",
       "      <td>Thank you! Thanks for having me over. I'm doin...</td>\n",
       "      <td>Ruben</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ruben Kogel on Self-Serve Analytics, R vs Pyt...</td>\n",
       "      <td>42</td>\n",
       "      <td>Business Data Science Database</td>\n",
       "      <td>Ruben Kogel</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Kirill: This is episode number one with ex-che...</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-09-10</td>\n",
       "      <td>2016</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>Database</td>\n",
       "      <td>Awesome.  It is great to hear you and for thos...</td>\n",
       "      <td>Kirill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ruben Kogel on Self-Serve Analytics, R vs Pyt...</td>\n",
       "      <td>42</td>\n",
       "      <td>Business Data Science Database</td>\n",
       "      <td>Ruben Kogel</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Kirill: This is episode number one with ex-che...</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-09-10</td>\n",
       "      <td>2016</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>Database</td>\n",
       "      <td>Yes sure. So, I'm the senior manager of analy...</td>\n",
       "      <td>Ruben</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49443</th>\n",
       "      <td>Contextual A.I. for Adapting to Adversaries, ...</td>\n",
       "      <td>81</td>\n",
       "      <td>Data Science Artificial Intelligence</td>\n",
       "      <td>Matar Haller</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Jon Krohn:\\t00:00:05\\tThis is episode number 6...</td>\n",
       "      <td>683</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Yes, right.  Yes, as I mean, it actually, it ...</td>\n",
       "      <td>Jon Krohn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49444</th>\n",
       "      <td>Contextual A.I. for Adapting to Adversaries, ...</td>\n",
       "      <td>81</td>\n",
       "      <td>Data Science Artificial Intelligence</td>\n",
       "      <td>Matar Haller</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Jon Krohn:\\t00:00:05\\tThis is episode number 6...</td>\n",
       "      <td>683</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Happy to.</td>\n",
       "      <td>Matar Haller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49445</th>\n",
       "      <td>Contextual A.I. for Adapting to Adversaries, ...</td>\n",
       "      <td>81</td>\n",
       "      <td>Data Science Artificial Intelligence</td>\n",
       "      <td>Matar Haller</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Jon Krohn:\\t00:00:05\\tThis is episode number 6...</td>\n",
       "      <td>683</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Nice. Well, yes, so you mentioned potentially ...</td>\n",
       "      <td>Jon Krohn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49446</th>\n",
       "      <td>Contextual A.I. for Adapting to Adversaries, ...</td>\n",
       "      <td>81</td>\n",
       "      <td>Data Science Artificial Intelligence</td>\n",
       "      <td>Matar Haller</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Jon Krohn:\\t00:00:05\\tThis is episode number 6...</td>\n",
       "      <td>683</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Thank you for having me. This was fascinating ...</td>\n",
       "      <td>Matar Haller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49447</th>\n",
       "      <td>Contextual A.I. for Adapting to Adversaries, ...</td>\n",
       "      <td>81</td>\n",
       "      <td>Data Science Artificial Intelligence</td>\n",
       "      <td>Matar Haller</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Jon Krohn:\\t00:00:05\\tThis is episode number 6...</td>\n",
       "      <td>683</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>I loved this conversation today. I hope you di...</td>\n",
       "      <td>Jon Krohn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49448 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            episode_name  length_episode  \\\n",
       "0       Ruben Kogel on Self-Serve Analytics, R vs Pyt...              42   \n",
       "1       Ruben Kogel on Self-Serve Analytics, R vs Pyt...              42   \n",
       "2       Ruben Kogel on Self-Serve Analytics, R vs Pyt...              42   \n",
       "3       Ruben Kogel on Self-Serve Analytics, R vs Pyt...              42   \n",
       "4       Ruben Kogel on Self-Serve Analytics, R vs Pyt...              42   \n",
       "...                                                  ...             ...   \n",
       "49443   Contextual A.I. for Adapting to Adversaries, ...              81   \n",
       "49444   Contextual A.I. for Adapting to Adversaries, ...              81   \n",
       "49445   Contextual A.I. for Adapting to Adversaries, ...              81   \n",
       "49446   Contextual A.I. for Adapting to Adversaries, ...              81   \n",
       "49447   Contextual A.I. for Adapting to Adversaries, ...              81   \n",
       "\n",
       "                            context_episode      guest_name  \\\n",
       "0            Business Data Science Database    Ruben Kogel    \n",
       "1            Business Data Science Database    Ruben Kogel    \n",
       "2            Business Data Science Database    Ruben Kogel    \n",
       "3            Business Data Science Database    Ruben Kogel    \n",
       "4            Business Data Science Database    Ruben Kogel    \n",
       "...                                     ...             ...   \n",
       "49443  Data Science Artificial Intelligence   Matar Haller    \n",
       "49444  Data Science Artificial Intelligence   Matar Haller    \n",
       "49445  Data Science Artificial Intelligence   Matar Haller    \n",
       "49446  Data Science Artificial Intelligence   Matar Haller    \n",
       "49447  Data Science Artificial Intelligence   Matar Haller    \n",
       "\n",
       "                                              guest_info  \\\n",
       "0      Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "1      Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "2      Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "3      Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "4      Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "...                                                  ...   \n",
       "49443  Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "49444  Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "49445  Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "49446  Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "49447  Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "\n",
       "                                            text_episode  episode_number  \\\n",
       "0      Kirill: This is episode number one with ex-che...               1   \n",
       "1      Kirill: This is episode number one with ex-che...               1   \n",
       "2      Kirill: This is episode number one with ex-che...               1   \n",
       "3      Kirill: This is episode number one with ex-che...               1   \n",
       "4      Kirill: This is episode number one with ex-che...               1   \n",
       "...                                                  ...             ...   \n",
       "49443  Jon Krohn:\\t00:00:05\\tThis is episode number 6...             683   \n",
       "49444  Jon Krohn:\\t00:00:05\\tThis is episode number 6...             683   \n",
       "49445  Jon Krohn:\\t00:00:05\\tThis is episode number 6...             683   \n",
       "49446  Jon Krohn:\\t00:00:05\\tThis is episode number 6...             683   \n",
       "49447  Jon Krohn:\\t00:00:05\\tThis is episode number 6...             683   \n",
       "\n",
       "      episode_date  episode_year episode_day     host_episode  \\\n",
       "0       2016-09-10          2016    Saturday  Kirill Eremenko   \n",
       "1       2016-09-10          2016    Saturday  Kirill Eremenko   \n",
       "2       2016-09-10          2016    Saturday  Kirill Eremenko   \n",
       "3       2016-09-10          2016    Saturday  Kirill Eremenko   \n",
       "4       2016-09-10          2016    Saturday  Kirill Eremenko   \n",
       "...            ...           ...         ...              ...   \n",
       "49443   2023-05-30          2023     Tuesday        Jon Krohn   \n",
       "49444   2023-05-30          2023     Tuesday        Jon Krohn   \n",
       "49445   2023-05-30          2023     Tuesday        Jon Krohn   \n",
       "49446   2023-05-30          2023     Tuesday        Jon Krohn   \n",
       "49447   2023-05-30          2023     Tuesday        Jon Krohn   \n",
       "\n",
       "                         class  \\\n",
       "0                     Database   \n",
       "1                     Database   \n",
       "2                     Database   \n",
       "3                     Database   \n",
       "4                     Database   \n",
       "...                        ...   \n",
       "49443  Artificial Intelligence   \n",
       "49444  Artificial Intelligence   \n",
       "49445  Artificial Intelligence   \n",
       "49446  Artificial Intelligence   \n",
       "49447  Artificial Intelligence   \n",
       "\n",
       "                                      episode_split_text       speaker  \n",
       "0      This is episode number one with ex-chemical en...        Kirill  \n",
       "1      Hey guys, welcome to the Podcast. I've got Rub...        Kirill  \n",
       "2      Thank you! Thanks for having me over. I'm doin...         Ruben  \n",
       "3      Awesome.  It is great to hear you and for thos...        Kirill  \n",
       "4       Yes sure. So, I'm the senior manager of analy...         Ruben  \n",
       "...                                                  ...           ...  \n",
       "49443   Yes, right.  Yes, as I mean, it actually, it ...     Jon Krohn  \n",
       "49444                                          Happy to.  Matar Haller  \n",
       "49445  Nice. Well, yes, so you mentioned potentially ...     Jon Krohn  \n",
       "49446  Thank you for having me. This was fascinating ...  Matar Haller  \n",
       "49447  I loved this conversation today. I hope you di...     Jon Krohn  \n",
       "\n",
       "[49448 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sds_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7266563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"They would come in between process and review. Because one of the key components of the review is data culling, and there are many ways you can do it. The traditional approach based on key words, they are in the past. Right now, especially if it is a large corporation, the cost is the most important factor. Because for many companies, it is a compliance issue. And if it is compliance, basically they want to do it cheap.You mentioned textual algorithms. There are different techniques which can help you isolate all non-responsive documents straight away before even starting the review. So you basically remove all these documents, and then there are certain techniques which can help you tell which data is not responsive even if you do not look at it. Basically, you pick a sample, you find the documents which are non-relevant, then you throw it back in the entire population, and then the system uses the algorithm to propagate that code, and you can tell which other documents might be relevant or not.In this world, you have to be extremely careful using those techniques. Because the cost is one side, but in the legal world, there is a requirement to review everything and not to miss a single bit. So there is no room for error.  That is why those algorithms exist, but not every company would use them. It would depend on the case. So sometimes you just need to make sure. You need to get 100% result. You have to make sure that everything has been reviewed. And if you do not get there, you just would not use those techniques.But there aren't many cases when it is a little bit relaxed, and you can used advanced algorithms and just get to the point quickly. There could be an error, but we are fine with that.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sds_data['episode_split_text'].loc[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c416f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text \n",
    "def lower_text(text):  \n",
    "    # Lowercase the text\n",
    "    if isinstance(text, float):\n",
    "        return text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation from the text\n",
    "    #text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Remove digits from the text\n",
    "    #text = ''.join(char for char in text if not char.isdigit())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b5096ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_data['episode_split_text'] = sds_data['episode_split_text'].apply(lower_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90d79b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_data.to_csv('../data/sds_BERTopic_split.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9c9f0",
   "metadata": {},
   "source": [
    "## Generating keywords for Data Science Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5d8088d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sds_ds_combined = pd.DataFrame(sds_data.groupby(['episode_number', 'episode_name', 'length_episode', 'class', 'guest_name', 'host_episode', 'episode_year', 'episode_date'])['episode_split_text'].agg(lambda x: ' '.join(str(i) for i in x))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a860f087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_number</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>length_episode</th>\n",
       "      <th>class</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>host_episode</th>\n",
       "      <th>episode_year</th>\n",
       "      <th>episode_date</th>\n",
       "      <th>episode_split_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ruben Kogel on Self-Serve Analytics, R vs Pyt...</td>\n",
       "      <td>42</td>\n",
       "      <td>Database</td>\n",
       "      <td>Ruben Kogel</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-09-10</td>\n",
       "      <td>this is episode number one with ex-chemical en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Machine Learning, Recommender Systems and the...</td>\n",
       "      <td>51</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Hadelin de Ponteves</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-09-14</td>\n",
       "      <td>this is session number two with machine learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Defining the Data Problem, Data Science in Ma...</td>\n",
       "      <td>53</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Dr. Wilson Pok</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-09-25</td>\n",
       "      <td>this is episode number three with nanophysics ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Data and Strategy, three Pillars of Research ...</td>\n",
       "      <td>60</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Brendan Hogan</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>this is episode four with business strategy ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Computer Forensics, Fraud Analytics and knowi...</td>\n",
       "      <td>63</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Dmitry Korneev</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-10-09</td>\n",
       "      <td>this is episode number five with forensics inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>679</td>\n",
       "      <td>The A.I. and Machine Learning Landscape, with...</td>\n",
       "      <td>94</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>George Mathew</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>this is episode number 679 with george matthew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>680</td>\n",
       "      <td>Automating Industrial Machines with Data Scie...</td>\n",
       "      <td>30</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Allegra Alessi</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>this is episode number 680 with allegra alessi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>681</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>72</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Matt Harrison</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>this is episode number 681 with matt harrison,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>682</td>\n",
       "      <td>Business Intelligence Tools, with Mico Yuk</td>\n",
       "      <td>28</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Mico Yuk</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-05-26</td>\n",
       "      <td>this is episode number 682 with mico yuk, host...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>683</td>\n",
       "      <td>Contextual A.I. for Adapting to Adversaries, ...</td>\n",
       "      <td>81</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Matar Haller</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>this is episode number 683 with dr. matar hall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>451 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     episode_number                                       episode_name  \\\n",
       "0                 1   Ruben Kogel on Self-Serve Analytics, R vs Pyt...   \n",
       "1                 2   Machine Learning, Recommender Systems and the...   \n",
       "2                 3   Defining the Data Problem, Data Science in Ma...   \n",
       "3                 4   Data and Strategy, three Pillars of Research ...   \n",
       "4                 5   Computer Forensics, Fraud Analytics and knowi...   \n",
       "..              ...                                                ...   \n",
       "446             679   The A.I. and Machine Learning Landscape, with...   \n",
       "447             680   Automating Industrial Machines with Data Scie...   \n",
       "448             681                                            XGBoost   \n",
       "449             682         Business Intelligence Tools, with Mico Yuk   \n",
       "450             683   Contextual A.I. for Adapting to Adversaries, ...   \n",
       "\n",
       "     length_episode                    class             guest_name  \\\n",
       "0                42                 Database           Ruben Kogel    \n",
       "1                51         Machine Learning   Hadelin de Ponteves    \n",
       "2                53         Machine Learning        Dr. Wilson Pok    \n",
       "3                60             Data Science         Brendan Hogan    \n",
       "4                63             Data Science        Dmitry Korneev    \n",
       "..              ...                      ...                    ...   \n",
       "446              94  Artificial Intelligence         George Mathew    \n",
       "447              30             Data Science        Allegra Alessi    \n",
       "448              72         Machine Learning         Matt Harrison    \n",
       "449              28             Data Science              Mico Yuk    \n",
       "450              81  Artificial Intelligence          Matar Haller    \n",
       "\n",
       "        host_episode  episode_year episode_date  \\\n",
       "0    Kirill Eremenko          2016   2016-09-10   \n",
       "1    Kirill Eremenko          2016   2016-09-14   \n",
       "2    Kirill Eremenko          2016   2016-09-25   \n",
       "3    Kirill Eremenko          2016   2016-10-02   \n",
       "4    Kirill Eremenko          2016   2016-10-09   \n",
       "..               ...           ...          ...   \n",
       "446        Jon Krohn          2023   2023-05-16   \n",
       "447        Jon Krohn          2023   2023-05-19   \n",
       "448        Jon Krohn          2023   2023-05-23   \n",
       "449        Jon Krohn          2023   2023-05-26   \n",
       "450        Jon Krohn          2023   2023-05-30   \n",
       "\n",
       "                                    episode_split_text  \n",
       "0    this is episode number one with ex-chemical en...  \n",
       "1    this is session number two with machine learni...  \n",
       "2    this is episode number three with nanophysics ...  \n",
       "3    this is episode four with business strategy ex...  \n",
       "4    this is episode number five with forensics inv...  \n",
       "..                                                 ...  \n",
       "446  this is episode number 679 with george matthew...  \n",
       "447  this is episode number 680 with allegra alessi...  \n",
       "448  this is episode number 681 with matt harrison,...  \n",
       "449  this is episode number 682 with mico yuk, host...  \n",
       "450  this is episode number 683 with dr. matar hall...  \n",
       "\n",
       "[451 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sds_ds_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391fedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove episode 202 and 546 with no text in episode_split_text\n",
    "#sds_processed = \n",
    "#sds_ds_processed[sds_ds_processed['episode_split_text'] == 'nan']#.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text \n",
    "def processed_text(text):  \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation from the text\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Remove digits from the text\n",
    "    text = ''.join(char for char in text if not char.isdigit())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef25bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['episode_split_text'].loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876eba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['episode_split_text'] = sds_ds_processed['episode_split_text'].apply(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea7982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c701bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22518b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['episode_split_text'].loc[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c02451",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['episode_keywords'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "105eff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_combined['episode_keywords'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4892f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate 100 keywords per episode using Yake library to store keywords in one column as list\n",
    "def yake_keyword_extractor(doc):\n",
    "    global sds_ds_combined\n",
    "    \n",
    "    for i, episode_text in doc.iteritems():\n",
    "        max_ngram_size = 3\n",
    "        deduplication_threshold = 0.3\n",
    "        windowSize = 1\n",
    "        numOfKeywords = 100\n",
    "        kw_extractor = yake.KeywordExtractor(n = max_ngram_size, dedupLim = deduplication_threshold , windowsSize = windowSize, top = numOfKeywords)\n",
    "        keywords = kw_extractor.extract_keywords(episode_text)\n",
    "        sds_ds_combined.at[i, 'episode_keywords'] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c804d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "yake_keyword_extractor(sds_ds_combined['episode_split_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yake_keyword_extractor(sds_ds_processed['episode_split_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_extractor(row):\n",
    "    words = [t[0].replace(\" \", \"_\") for t in row]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['keywords'] = sds_ds_processed['episode_keywords'].apply(tuple_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed.to_csv('../data/sds_ds_keywords.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c4323",
   "metadata": {},
   "source": [
    "## Generating keywords for non-Data Science Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed = pd.DataFrame(sds_non_data.groupby(['episode_number', 'episode_name', 'length_episode', 'context_episode', 'guest_name', 'host_episode', 'episode_year'])['episode_split_text'].agg(lambda x: ' '.join(str(i) for i in x))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed['episode_split_text'] = sds_nds_processed['episode_split_text'].apply(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bbf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed['episode_keywords'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06541a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate 100 keywords per episode using Yake library to store keywords in one column as list\n",
    "def yake_keyword_extractor(doc):\n",
    "    global sds_nds_processed\n",
    "    \n",
    "    for i, episode_text in doc.iteritems():\n",
    "        max_ngram_size = 3\n",
    "        deduplication_threshold = 0.3\n",
    "        windowSize = 1\n",
    "        numOfKeywords = 100\n",
    "        kw_extractor = yake.KeywordExtractor(n = max_ngram_size, dedupLim = deduplication_threshold , windowsSize = windowSize, top = numOfKeywords)\n",
    "        keywords = kw_extractor.extract_keywords(episode_text)\n",
    "        sds_nds_processed.at[i, 'episode_keywords'] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "yake_keyword_extractor(sds_nds_processed['episode_split_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7427f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed['keywords'] = sds_nds_processed['episode_keywords'].apply(tuple_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed.to_csv('../data/sds_nds_keywords.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62198e54",
   "metadata": {},
   "source": [
    "### Count Vectorizer and Class-Based Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b326cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_keywords = pd.read_csv('../data/sds_ds_keywords.csv')\n",
    "\n",
    "keywords_per_class = sds_ds_keywords.groupby('class')['keywords'].apply(' '.join).reset_index()\n",
    "\n",
    "# Rename the columns to match the original code\n",
    "keywords_per_class = keywords_per_class.rename(columns={'class': 'Class', 'keywords': 'Document'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eebfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTFIDFVectorizer(TfidfTransformer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CTFIDFVectorizer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def fit(self, X: sp.csr_matrix, n_samples: int):\n",
    "        \"\"\"Learn the idf vector (global term weights) \"\"\"\n",
    "        _, n_features = X.shape\n",
    "        df = np.squeeze(np.asarray(X.sum(axis=0)))\n",
    "        idf = np.log(n_samples / df)\n",
    "        self._idf_diag = sp.diags(idf, offsets=0,\n",
    "                                  shape=(n_features, n_features),\n",
    "                                  format='csr',\n",
    "                                  dtype=np.float64)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: sp.csr_matrix) -> sp.csr_matrix:\n",
    "        \"\"\"Transform a count-based matrix to c-TF-IDF \"\"\"\n",
    "        X = X * self._idf_diag\n",
    "        X = normalize(X, axis=1, norm='l1', copy=False)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create c-TF-IDF\n",
    "count = CountVectorizer().fit_transform(keywords_per_class.Document)\n",
    "ctfidf = CTFIDFVectorizer().fit_transform(count, n_samples=len(keywords_per_class)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag of words using count vectorizer\n",
    "count_vectorizer = CountVectorizer().fit(keywords_per_class.Document)\n",
    "count = count_vectorizer.transform(keywords_per_class.Document)\n",
    "words = count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d1fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc9b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9716a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = pd.DataFrame({\n",
    "    'words': count_vectorizer.get_feature_names_out(),\n",
    "    'frequency': np.array(count.sum(axis = 0)).flatten()\n",
    "})\n",
    "word_counts.sort_values('frequency', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts[word_counts['frequency'] < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class based tfidf\n",
    "ctfidf = CTFIDFVectorizer().fit_transform(count, n_samples=len(keywords_per_class)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156230e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique words in each class\n",
    "unique_words_per_class = []\n",
    "for i, class_docs in enumerate(keywords_per_class.Document):\n",
    "    tfidf_scores = ctfidf[i]\n",
    "    unique_word_indices = np.argsort(tfidf_scores)[-50:][::-1]  # index\n",
    "    unique_words = [words[idx] for idx in unique_word_indices]  # feature name\n",
    "    unique_words_per_class.append(unique_words)\n",
    "\n",
    "class_unique_50 = pd.DataFrame({'Class': keywords_per_class.Class, 'unique_Words': unique_words_per_class})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1089ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_50['unique_Words'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cccd274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_50['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603207e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_50.to_csv('../data/keywords_u20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed37514",
   "metadata": {},
   "source": [
    "### Generate word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68500d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_keywords = ' '.join(keywords_per_class.Document)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_keywords)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = ' '.join(map(str, class_unique_50.unique_Words))\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_keywords)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1549587",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_bert = pd.read_csv('../data/sds_ds_keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_bert[\"episode_keywords\"] = None\n",
    "sds_ds_bert[\"keywords\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f232e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_keyword_extractor(doc):\n",
    "    kw_model = KeyBERT()\n",
    "    global sds_ds_bert\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    \n",
    "    for i, episode_text in doc.iteritems():\n",
    "        keywords = kw_model.extract_keywords(episode_text, keyphrase_ngram_range=(1, 3), stop_words = stopwords, top_n = 100, use_mmr=True, diversity=0.7)\n",
    "        sds_ds_bert.at[i, 'episode_keywords'] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_keyword_extractor(sds_ds_bert['episode_split_text'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_bert['episode_keywords'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffee6d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://maartengr.github.io/BERTopic/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
