{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2f934c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from keybert import KeyBERT\n",
    "import yake\n",
    "import string\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a613e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sds_data = pd.read_csv('../data/sds_ds_text.csv')\n",
    "sds_non_data = pd.read_csv('../data/sds_nds_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9c9f0",
   "metadata": {},
   "source": [
    "## Generating keywords for Data Science Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d8088d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sds_ds_processed = pd.DataFrame(sds_data.groupby(['episode_number', 'episode_name', 'length_episode', 'class', 'guest_name', 'host_episode', 'episode_year', 'episode_date'])['episode_split_text'].agg(lambda x: ' '.join(str(i) for i in x))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391fedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove episode 202 and 546 with no text in episode_split_text\n",
    "#sds_processed = \n",
    "#sds_ds_processed[sds_ds_processed['episode_split_text'] == 'nan']#.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "826d945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text \n",
    "def processed_text(text):  \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation from the text\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Remove digits from the text\n",
    "    text = ''.join(char for char in text if not char.isdigit())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "876eba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['episode_split_text'] = sds_ds_processed['episode_split_text'].apply(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac3b104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_number</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>length_episode</th>\n",
       "      <th>class</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>host_episode</th>\n",
       "      <th>episode_year</th>\n",
       "      <th>episode_date</th>\n",
       "      <th>episode_split_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ruben Kogel on Self-Serve Analytics, R vs Pyt...</td>\n",
       "      <td>42</td>\n",
       "      <td>Database</td>\n",
       "      <td>Ruben Kogel</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>Sep 10, 2016</td>\n",
       "      <td>this is episode number one with exchemical eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Machine Learning, Recommender Systems and the...</td>\n",
       "      <td>51</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Hadelin de Ponteves</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>Sep 14, 2016</td>\n",
       "      <td>this is session number two with machine learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Defining the Data Problem, Data Science in Ma...</td>\n",
       "      <td>53</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Dr. Wilson Pok</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>Sep 25, 2016</td>\n",
       "      <td>this is episode number three with nanophysics ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Data and Strategy, three Pillars of Research ...</td>\n",
       "      <td>60</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Brendan Hogan</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>Oct 02, 2016</td>\n",
       "      <td>this is episode four with business strategy ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Computer Forensics, Fraud Analytics and knowi...</td>\n",
       "      <td>63</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Dmitry Korneev</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>Oct 09, 2016</td>\n",
       "      <td>this is episode number five with forensics inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>679</td>\n",
       "      <td>The A.I. and Machine Learning Landscape, with...</td>\n",
       "      <td>94</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>George Mathew</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>May 16, 2023</td>\n",
       "      <td>this is episode number  with george matthew ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>680</td>\n",
       "      <td>Automating Industrial Machines with Data Scie...</td>\n",
       "      <td>30</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Allegra Alessi</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>May 19, 2023</td>\n",
       "      <td>this is episode number  with allegra alessi io...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>681</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>72</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Matt Harrison</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>May 23, 2023</td>\n",
       "      <td>this is episode number  with matt harrison man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>682</td>\n",
       "      <td>Business Intelligence Tools, with Mico Yuk</td>\n",
       "      <td>28</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Mico Yuk</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>this is episode number  with mico yuk host of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>683</td>\n",
       "      <td>Contextual A.I. for Adapting to Adversaries, ...</td>\n",
       "      <td>81</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Matar Haller</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>May 30, 2023</td>\n",
       "      <td>this is episode number  with dr matar haller v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>451 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     episode_number                                       episode_name  \\\n",
       "0                 1   Ruben Kogel on Self-Serve Analytics, R vs Pyt...   \n",
       "1                 2   Machine Learning, Recommender Systems and the...   \n",
       "2                 3   Defining the Data Problem, Data Science in Ma...   \n",
       "3                 4   Data and Strategy, three Pillars of Research ...   \n",
       "4                 5   Computer Forensics, Fraud Analytics and knowi...   \n",
       "..              ...                                                ...   \n",
       "446             679   The A.I. and Machine Learning Landscape, with...   \n",
       "447             680   Automating Industrial Machines with Data Scie...   \n",
       "448             681                                            XGBoost   \n",
       "449             682         Business Intelligence Tools, with Mico Yuk   \n",
       "450             683   Contextual A.I. for Adapting to Adversaries, ...   \n",
       "\n",
       "     length_episode                    class             guest_name  \\\n",
       "0                42                 Database           Ruben Kogel    \n",
       "1                51         Machine Learning   Hadelin de Ponteves    \n",
       "2                53         Machine Learning        Dr. Wilson Pok    \n",
       "3                60             Data Science         Brendan Hogan    \n",
       "4                63             Data Science        Dmitry Korneev    \n",
       "..              ...                      ...                    ...   \n",
       "446              94  Artificial Intelligence         George Mathew    \n",
       "447              30             Data Science        Allegra Alessi    \n",
       "448              72         Machine Learning         Matt Harrison    \n",
       "449              28             Data Science              Mico Yuk    \n",
       "450              81  Artificial Intelligence          Matar Haller    \n",
       "\n",
       "        host_episode  episode_year  episode_date  \\\n",
       "0    Kirill Eremenko          2016  Sep 10, 2016   \n",
       "1    Kirill Eremenko          2016  Sep 14, 2016   \n",
       "2    Kirill Eremenko          2016  Sep 25, 2016   \n",
       "3    Kirill Eremenko          2016  Oct 02, 2016   \n",
       "4    Kirill Eremenko          2016  Oct 09, 2016   \n",
       "..               ...           ...           ...   \n",
       "446        Jon Krohn          2023  May 16, 2023   \n",
       "447        Jon Krohn          2023  May 19, 2023   \n",
       "448        Jon Krohn          2023  May 23, 2023   \n",
       "449        Jon Krohn          2023  May 26, 2023   \n",
       "450        Jon Krohn          2023  May 30, 2023   \n",
       "\n",
       "                                    episode_split_text  \n",
       "0    this is episode number one with exchemical eng...  \n",
       "1    this is session number two with machine learni...  \n",
       "2    this is episode number three with nanophysics ...  \n",
       "3    this is episode four with business strategy ex...  \n",
       "4    this is episode number five with forensics inv...  \n",
       "..                                                 ...  \n",
       "446  this is episode number  with george matthew ma...  \n",
       "447  this is episode number  with allegra alessi io...  \n",
       "448  this is episode number  with matt harrison man...  \n",
       "449  this is episode number  with mico yuk host of ...  \n",
       "450  this is episode number  with dr matar haller v...  \n",
       "\n",
       "[451 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sds_ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "121b9a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sds_ds_processed['episode_date'] = pd.to_datetime(sds_ds_processed['episode_date'], format='%b %d, %Y' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7382df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['episode_year'] = sds_ds_processed['episode_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4915e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_number</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>length_episode</th>\n",
       "      <th>class</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>host_episode</th>\n",
       "      <th>episode_year</th>\n",
       "      <th>episode_date</th>\n",
       "      <th>episode_split_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ruben Kogel on Self-Serve Analytics, R vs Pyt...</td>\n",
       "      <td>42</td>\n",
       "      <td>Database</td>\n",
       "      <td>Ruben Kogel</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-09-10</td>\n",
       "      <td>this is episode number one with exchemical eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Machine Learning, Recommender Systems and the...</td>\n",
       "      <td>51</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Hadelin de Ponteves</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-09-14</td>\n",
       "      <td>this is session number two with machine learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Defining the Data Problem, Data Science in Ma...</td>\n",
       "      <td>53</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Dr. Wilson Pok</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-09-25</td>\n",
       "      <td>this is episode number three with nanophysics ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Data and Strategy, three Pillars of Research ...</td>\n",
       "      <td>60</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Brendan Hogan</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>this is episode four with business strategy ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Computer Forensics, Fraud Analytics and knowi...</td>\n",
       "      <td>63</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Dmitry Korneev</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-10-09</td>\n",
       "      <td>this is episode number five with forensics inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>679</td>\n",
       "      <td>The A.I. and Machine Learning Landscape, with...</td>\n",
       "      <td>94</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>George Mathew</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>this is episode number  with george matthew ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>680</td>\n",
       "      <td>Automating Industrial Machines with Data Scie...</td>\n",
       "      <td>30</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Allegra Alessi</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>this is episode number  with allegra alessi io...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>681</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>72</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Matt Harrison</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>this is episode number  with matt harrison man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>682</td>\n",
       "      <td>Business Intelligence Tools, with Mico Yuk</td>\n",
       "      <td>28</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Mico Yuk</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-05-26</td>\n",
       "      <td>this is episode number  with mico yuk host of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>683</td>\n",
       "      <td>Contextual A.I. for Adapting to Adversaries, ...</td>\n",
       "      <td>81</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Matar Haller</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>this is episode number  with dr matar haller v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>451 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     episode_number                                       episode_name  \\\n",
       "0                 1   Ruben Kogel on Self-Serve Analytics, R vs Pyt...   \n",
       "1                 2   Machine Learning, Recommender Systems and the...   \n",
       "2                 3   Defining the Data Problem, Data Science in Ma...   \n",
       "3                 4   Data and Strategy, three Pillars of Research ...   \n",
       "4                 5   Computer Forensics, Fraud Analytics and knowi...   \n",
       "..              ...                                                ...   \n",
       "446             679   The A.I. and Machine Learning Landscape, with...   \n",
       "447             680   Automating Industrial Machines with Data Scie...   \n",
       "448             681                                            XGBoost   \n",
       "449             682         Business Intelligence Tools, with Mico Yuk   \n",
       "450             683   Contextual A.I. for Adapting to Adversaries, ...   \n",
       "\n",
       "     length_episode                    class             guest_name  \\\n",
       "0                42                 Database           Ruben Kogel    \n",
       "1                51         Machine Learning   Hadelin de Ponteves    \n",
       "2                53         Machine Learning        Dr. Wilson Pok    \n",
       "3                60             Data Science         Brendan Hogan    \n",
       "4                63             Data Science        Dmitry Korneev    \n",
       "..              ...                      ...                    ...   \n",
       "446              94  Artificial Intelligence         George Mathew    \n",
       "447              30             Data Science        Allegra Alessi    \n",
       "448              72         Machine Learning         Matt Harrison    \n",
       "449              28             Data Science              Mico Yuk    \n",
       "450              81  Artificial Intelligence          Matar Haller    \n",
       "\n",
       "        host_episode  episode_year episode_date  \\\n",
       "0    Kirill Eremenko          2016   2016-09-10   \n",
       "1    Kirill Eremenko          2016   2016-09-14   \n",
       "2    Kirill Eremenko          2016   2016-09-25   \n",
       "3    Kirill Eremenko          2016   2016-10-02   \n",
       "4    Kirill Eremenko          2016   2016-10-09   \n",
       "..               ...           ...          ...   \n",
       "446        Jon Krohn          2023   2023-05-16   \n",
       "447        Jon Krohn          2023   2023-05-19   \n",
       "448        Jon Krohn          2023   2023-05-23   \n",
       "449        Jon Krohn          2023   2023-05-26   \n",
       "450        Jon Krohn          2023   2023-05-30   \n",
       "\n",
       "                                    episode_split_text  \n",
       "0    this is episode number one with exchemical eng...  \n",
       "1    this is session number two with machine learni...  \n",
       "2    this is episode number three with nanophysics ...  \n",
       "3    this is episode four with business strategy ex...  \n",
       "4    this is episode number five with forensics inv...  \n",
       "..                                                 ...  \n",
       "446  this is episode number  with george matthew ma...  \n",
       "447  this is episode number  with allegra alessi io...  \n",
       "448  this is episode number  with matt harrison man...  \n",
       "449  this is episode number  with mico yuk host of ...  \n",
       "450  this is episode number  with dr matar haller v...  \n",
       "\n",
       "[451 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sds_ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e5fad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 451 entries, 0 to 450\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   episode_number      451 non-null    int64         \n",
      " 1   episode_name        451 non-null    object        \n",
      " 2   length_episode      451 non-null    int64         \n",
      " 3   class               451 non-null    object        \n",
      " 4   guest_name          451 non-null    object        \n",
      " 5   host_episode        451 non-null    object        \n",
      " 6   episode_year        451 non-null    int64         \n",
      " 7   episode_date        451 non-null    datetime64[ns]\n",
      " 8   episode_split_text  451 non-null    object        \n",
      "dtypes: datetime64[ns](1), int64(3), object(5)\n",
      "memory usage: 31.8+ KB\n"
     ]
    }
   ],
   "source": [
    "sds_ds_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22518b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['episode_split_text'].loc[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c02451",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['episode_keywords'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate 100 keywords per episode using Yake library to store keywords in one column as list\n",
    "def yake_keyword_extractor(doc):\n",
    "    global sds_ds_processed\n",
    "    \n",
    "    for i, episode_text in doc.iteritems():\n",
    "        max_ngram_size = 3\n",
    "        deduplication_threshold = 0.3\n",
    "        windowSize = 1\n",
    "        numOfKeywords = 100\n",
    "        kw_extractor = yake.KeywordExtractor(n = max_ngram_size, dedupLim = deduplication_threshold , windowsSize = windowSize, top = numOfKeywords)\n",
    "        keywords = kw_extractor.extract_keywords(episode_text)\n",
    "        sds_ds_processed.at[i, 'episode_keywords'] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yake_keyword_extractor(sds_ds_processed['episode_split_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_extractor(row):\n",
    "    words = [t[0].replace(\" \", \"_\") for t in row]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['keywords'] = sds_ds_processed['episode_keywords'].apply(tuple_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed.to_csv('../data/sds_ds_keywords.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c4323",
   "metadata": {},
   "source": [
    "## Generating keywords for non-Data Science Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed = pd.DataFrame(sds_non_data.groupby(['episode_number', 'episode_name', 'length_episode', 'context_episode', 'guest_name', 'host_episode', 'episode_year'])['episode_split_text'].agg(lambda x: ' '.join(str(i) for i in x))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed['episode_split_text'] = sds_nds_processed['episode_split_text'].apply(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bbf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed['episode_keywords'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06541a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate 100 keywords per episode using Yake library to store keywords in one column as list\n",
    "def yake_keyword_extractor(doc):\n",
    "    global sds_nds_processed\n",
    "    \n",
    "    for i, episode_text in doc.iteritems():\n",
    "        max_ngram_size = 3\n",
    "        deduplication_threshold = 0.3\n",
    "        windowSize = 1\n",
    "        numOfKeywords = 100\n",
    "        kw_extractor = yake.KeywordExtractor(n = max_ngram_size, dedupLim = deduplication_threshold , windowsSize = windowSize, top = numOfKeywords)\n",
    "        keywords = kw_extractor.extract_keywords(episode_text)\n",
    "        sds_nds_processed.at[i, 'episode_keywords'] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "yake_keyword_extractor(sds_nds_processed['episode_split_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7427f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed['keywords'] = sds_nds_processed['episode_keywords'].apply(tuple_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed.to_csv('../data/sds_nds_keywords.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62198e54",
   "metadata": {},
   "source": [
    "### Count Vectorizer and Class-Based Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b326cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_keywords = pd.read_csv('../data/sds_ds_keywords.csv')\n",
    "\n",
    "keywords_per_class = t\n",
    "\n",
    "# Rename the columns to match the original code\n",
    "keywords_per_class = keywords_per_class.rename(columns={'class': 'Class', 'keywords': 'Document'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eebfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTFIDFVectorizer(TfidfTransformer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CTFIDFVectorizer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def fit(self, X: sp.csr_matrix, n_samples: int):\n",
    "        \"\"\"Learn the idf vector (global term weights) \"\"\"\n",
    "        _, n_features = X.shape\n",
    "        df = np.squeeze(np.asarray(X.sum(axis=0)))\n",
    "        idf = np.log(n_samples / df)\n",
    "        self._idf_diag = sp.diags(idf, offsets=0,\n",
    "                                  shape=(n_features, n_features),\n",
    "                                  format='csr',\n",
    "                                  dtype=np.float64)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: sp.csr_matrix) -> sp.csr_matrix:\n",
    "        \"\"\"Transform a count-based matrix to c-TF-IDF \"\"\"\n",
    "        X = X * self._idf_diag\n",
    "        X = normalize(X, axis=1, norm='l1', copy=False)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create c-TF-IDF\n",
    "count = CountVectorizer().fit_transform(keywords_per_class.Document)\n",
    "ctfidf = CTFIDFVectorizer().fit_transform(count, n_samples=len(keywords_per_class)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag of words using count vectorizer\n",
    "count_vectorizer = CountVectorizer().fit(keywords_per_class.Document)\n",
    "count = count_vectorizer.transform(keywords_per_class.Document)\n",
    "words = count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d1fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc9b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9716a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = pd.DataFrame({\n",
    "    'words': count_vectorizer.get_feature_names_out(),\n",
    "    'frequency': np.array(count.sum(axis = 0)).flatten()\n",
    "})\n",
    "word_counts.sort_values('frequency', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts[word_counts['frequency'] < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class based tfidf\n",
    "ctfidf = CTFIDFVectorizer().fit_transform(count, n_samples=len(keywords_per_class)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156230e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique words in each class\n",
    "unique_words_per_class = []\n",
    "for i, class_docs in enumerate(keywords_per_class.Document):\n",
    "    tfidf_scores = ctfidf[i]\n",
    "    unique_word_indices = np.argsort(tfidf_scores)[-20:][::-1]  # index\n",
    "    unique_words = [words[idx] for idx in unique_word_indices]  # feature name\n",
    "    unique_words_per_class.append(unique_words)\n",
    "\n",
    "class_unique_25 = pd.DataFrame({'Class': keywords_per_class.Class, 'unique_Words': unique_words_per_class})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1089ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_20['unique_Words'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cccd274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_20['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603207e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_20.to_csv('../data/keywords_u20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed37514",
   "metadata": {},
   "source": [
    "### Generate word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68500d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_keywords = ' '.join(keywords_per_class.Document)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_keywords)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = ' '.join(map(str, class_unique_25.unique_Words))\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_keywords)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['episode_keywords'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_keyword_extractor(doc):\n",
    "    kw_model = KeyBERT()\n",
    "    global sds_ds_processed\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    \n",
    "    for i, episode_text in doc.iteritems():\n",
    "        keywords = kw_model.extract_keywords(episode_text, keyphrase_ngram_range=(1, 3), stop_words = stopwords, top_n = 100, use_mmr=True, diversity=0.7)\n",
    "        sds_ds_processed.at[i, 'episode_keywords'] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_keyword_extractor(sds_ds_processed['episode_split_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffee6d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://maartengr.github.io/BERTopic/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
