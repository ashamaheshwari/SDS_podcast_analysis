{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f934c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from keybert import KeyBERT\n",
    "import yake\n",
    "import string\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a613e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_data = pd.read_csv('../data/sds_ds_text.csv')\n",
    "sds_non_data = pd.read_csv('../data/sds_nds_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9c9f0",
   "metadata": {},
   "source": [
    "## Generating keywords for Data Science Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed = pd.DataFrame(sds_data.groupby(['episode_number', 'episode_name', 'length_episode', 'class', 'guest_name', 'host_episode', 'episode_year', 'episode_date'])['episode_split_text'].agg(lambda x: ' '.join(str(i) for i in x))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391fedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove episode 202 and 546 with no text in episode_split_text\n",
    "#sds_processed = \n",
    "#sds_ds_processed[sds_ds_processed['episode_split_text'] == 'nan']#.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text \n",
    "def processed_text(text):  \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation from the text\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Remove digits from the text\n",
    "    text = ''.join(char for char in text if not char.isdigit())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876eba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['episode_split_text'] = sds_ds_processed['episode_split_text'].apply(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2bdd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22518b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['episode_split_text'].loc[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c02451",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['episode_keywords'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate 100 keywords per episode using Yake library to store keywords in one column as list\n",
    "def yake_keyword_extractor(doc):\n",
    "    global sds_ds_processed\n",
    "    \n",
    "    for i, episode_text in doc.iteritems():\n",
    "        max_ngram_size = 3\n",
    "        deduplication_threshold = 0.3\n",
    "        windowSize = 1\n",
    "        numOfKeywords = 100\n",
    "        kw_extractor = yake.KeywordExtractor(n = max_ngram_size, dedupLim = deduplication_threshold , windowsSize = windowSize, top = numOfKeywords)\n",
    "        keywords = kw_extractor.extract_keywords(episode_text)\n",
    "        sds_ds_processed.at[i, 'episode_keywords'] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yake_keyword_extractor(sds_ds_processed['episode_split_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_extractor(row):\n",
    "    words = [t[0].replace(\" \", \"_\") for t in row]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['keywords'] = sds_ds_processed['episode_keywords'].apply(tuple_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed.to_csv('../data/sds_ds_keywords.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c4323",
   "metadata": {},
   "source": [
    "## Generating keywords for non-Data Science Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed = pd.DataFrame(sds_non_data.groupby(['episode_number', 'episode_name', 'length_episode', 'context_episode', 'guest_name', 'host_episode', 'episode_year'])['episode_split_text'].agg(lambda x: ' '.join(str(i) for i in x))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed['episode_split_text'] = sds_nds_processed['episode_split_text'].apply(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bbf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed['episode_keywords'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06541a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate 100 keywords per episode using Yake library to store keywords in one column as list\n",
    "def yake_keyword_extractor(doc):\n",
    "    global sds_nds_processed\n",
    "    \n",
    "    for i, episode_text in doc.iteritems():\n",
    "        max_ngram_size = 3\n",
    "        deduplication_threshold = 0.3\n",
    "        windowSize = 1\n",
    "        numOfKeywords = 100\n",
    "        kw_extractor = yake.KeywordExtractor(n = max_ngram_size, dedupLim = deduplication_threshold , windowsSize = windowSize, top = numOfKeywords)\n",
    "        keywords = kw_extractor.extract_keywords(episode_text)\n",
    "        sds_nds_processed.at[i, 'episode_keywords'] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "yake_keyword_extractor(sds_nds_processed['episode_split_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7427f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed['keywords'] = sds_nds_processed['episode_keywords'].apply(tuple_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed.to_csv('../data/sds_nds_keywords.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62198e54",
   "metadata": {},
   "source": [
    "### Count Vectorizer and Class-Based Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b326cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_keywords = pd.read_csv('../data/sds_ds_keywords.csv')\n",
    "\n",
    "keywords_per_class = t\n",
    "\n",
    "# Rename the columns to match the original code\n",
    "keywords_per_class = keywords_per_class.rename(columns={'class': 'Class', 'keywords': 'Document'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eebfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTFIDFVectorizer(TfidfTransformer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CTFIDFVectorizer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def fit(self, X: sp.csr_matrix, n_samples: int):\n",
    "        \"\"\"Learn the idf vector (global term weights) \"\"\"\n",
    "        _, n_features = X.shape\n",
    "        df = np.squeeze(np.asarray(X.sum(axis=0)))\n",
    "        idf = np.log(n_samples / df)\n",
    "        self._idf_diag = sp.diags(idf, offsets=0,\n",
    "                                  shape=(n_features, n_features),\n",
    "                                  format='csr',\n",
    "                                  dtype=np.float64)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: sp.csr_matrix) -> sp.csr_matrix:\n",
    "        \"\"\"Transform a count-based matrix to c-TF-IDF \"\"\"\n",
    "        X = X * self._idf_diag\n",
    "        X = normalize(X, axis=1, norm='l1', copy=False)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create c-TF-IDF\n",
    "count = CountVectorizer().fit_transform(keywords_per_class.Document)\n",
    "ctfidf = CTFIDFVectorizer().fit_transform(count, n_samples=len(keywords_per_class)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag of words using count vectorizer\n",
    "count_vectorizer = CountVectorizer().fit(keywords_per_class.Document)\n",
    "count = count_vectorizer.transform(keywords_per_class.Document)\n",
    "words = count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d1fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc9b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9716a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = pd.DataFrame({\n",
    "    'words': count_vectorizer.get_feature_names_out(),\n",
    "    'frequency': np.array(count.sum(axis = 0)).flatten()\n",
    "})\n",
    "word_counts.sort_values('frequency', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts[word_counts['frequency'] < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class based tfidf\n",
    "ctfidf = CTFIDFVectorizer().fit_transform(count, n_samples=len(keywords_per_class)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156230e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique words in each class\n",
    "unique_words_per_class = []\n",
    "for i, class_docs in enumerate(keywords_per_class.Document):\n",
    "    tfidf_scores = ctfidf[i]\n",
    "    unique_word_indices = np.argsort(tfidf_scores)[-20:][::-1]  # index\n",
    "    unique_words = [words[idx] for idx in unique_word_indices]  # feature name\n",
    "    unique_words_per_class.append(unique_words)\n",
    "\n",
    "class_unique_25 = pd.DataFrame({'Class': keywords_per_class.Class, 'unique_Words': unique_words_per_class})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1089ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_20['unique_Words'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cccd274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_20['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603207e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_20.to_csv('../data/keywords_u20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed37514",
   "metadata": {},
   "source": [
    "### Generate word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68500d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_keywords = ' '.join(keywords_per_class.Document)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_keywords)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = ' '.join(map(str, class_unique_25.unique_Words))\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_keywords)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['episode_keywords'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_keyword_extractor(doc):\n",
    "    kw_model = KeyBERT()\n",
    "    global sds_ds_processed\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    \n",
    "    for i, episode_text in doc.iteritems():\n",
    "        keywords = kw_model.extract_keywords(episode_text, keyphrase_ngram_range=(1, 3), stop_words = stopwords, top_n = 100, use_mmr=True, diversity=0.7)\n",
    "        sds_ds_processed.at[i, 'episode_keywords'] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_keyword_extractor(sds_ds_processed['episode_split_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffee6d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://maartengr.github.io/BERTopic/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
