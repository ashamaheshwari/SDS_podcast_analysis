{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f934c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "#from keybert import KeyBERT\n",
    "import yake\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a613e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sds_data = pd.read_csv('../data/sds_ds_text.csv')\n",
    "sds_non_data = pd.read_csv('../data/sds_nds_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9c9f0",
   "metadata": {},
   "source": [
    "## Generating keywords for Data Science Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d8088d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sds_ds_processed = pd.DataFrame(sds_data.groupby(['episode_number', 'episode_name', 'length_episode', 'class', 'guest_name', 'host_episode', 'episode_year'])['episode_split_text'].agg(lambda x: ' '.join(str(i) for i in x))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41885528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_number</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>length_episode</th>\n",
       "      <th>class</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>host_episode</th>\n",
       "      <th>episode_year</th>\n",
       "      <th>episode_split_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ruben Kogel on Self-Serve Analytics, R vs Pyt...</td>\n",
       "      <td>42</td>\n",
       "      <td>Database</td>\n",
       "      <td>Ruben Kogel</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>This is episode number one with ex-chemical en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Machine Learning, Recommender Systems and the...</td>\n",
       "      <td>51</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Hadelin de Ponteves</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>This is session number two with machine learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Defining the Data Problem, Data Science in Ma...</td>\n",
       "      <td>53</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Dr. Wilson Pok</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>This is episode number three with Nanophysics ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Data and Strategy, three Pillars of Research ...</td>\n",
       "      <td>60</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Brendan Hogan</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>This is episode four with business strategy ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Computer Forensics, Fraud Analytics and knowi...</td>\n",
       "      <td>63</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Dmitry Korneev</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "      <td>2016</td>\n",
       "      <td>This is episode number five with forensics inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>679</td>\n",
       "      <td>The A.I. and Machine Learning Landscape, with...</td>\n",
       "      <td>94</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>George Mathew</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>This is episode number 679 with George Matthew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>680</td>\n",
       "      <td>Automating Industrial Machines with Data Scie...</td>\n",
       "      <td>30</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Allegra Alessi</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>This is episode number 680 with Allegra Alessi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>681</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>72</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Matt Harrison</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>This is episode number 681 with Matt Harrison,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>682</td>\n",
       "      <td>Business Intelligence Tools, with Mico Yuk</td>\n",
       "      <td>28</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Mico Yuk</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>This is episode number 682 with Mico Yuk, host...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>683</td>\n",
       "      <td>Contextual A.I. for Adapting to Adversaries, ...</td>\n",
       "      <td>81</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Matar Haller</td>\n",
       "      <td>Jon Krohn</td>\n",
       "      <td>2023</td>\n",
       "      <td>This is episode number 683 with Dr. Matar Hall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>451 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     episode_number                                       episode_name  \\\n",
       "0                 1   Ruben Kogel on Self-Serve Analytics, R vs Pyt...   \n",
       "1                 2   Machine Learning, Recommender Systems and the...   \n",
       "2                 3   Defining the Data Problem, Data Science in Ma...   \n",
       "3                 4   Data and Strategy, three Pillars of Research ...   \n",
       "4                 5   Computer Forensics, Fraud Analytics and knowi...   \n",
       "..              ...                                                ...   \n",
       "446             679   The A.I. and Machine Learning Landscape, with...   \n",
       "447             680   Automating Industrial Machines with Data Scie...   \n",
       "448             681                                            XGBoost   \n",
       "449             682         Business Intelligence Tools, with Mico Yuk   \n",
       "450             683   Contextual A.I. for Adapting to Adversaries, ...   \n",
       "\n",
       "     length_episode                    class             guest_name  \\\n",
       "0                42                 Database           Ruben Kogel    \n",
       "1                51         Machine Learning   Hadelin de Ponteves    \n",
       "2                53         Machine Learning        Dr. Wilson Pok    \n",
       "3                60             Data Science         Brendan Hogan    \n",
       "4                63             Data Science        Dmitry Korneev    \n",
       "..              ...                      ...                    ...   \n",
       "446              94  Artificial Intelligence         George Mathew    \n",
       "447              30             Data Science        Allegra Alessi    \n",
       "448              72         Machine Learning         Matt Harrison    \n",
       "449              28             Data Science              Mico Yuk    \n",
       "450              81  Artificial Intelligence          Matar Haller    \n",
       "\n",
       "        host_episode  episode_year  \\\n",
       "0    Kirill Eremenko          2016   \n",
       "1    Kirill Eremenko          2016   \n",
       "2    Kirill Eremenko          2016   \n",
       "3    Kirill Eremenko          2016   \n",
       "4    Kirill Eremenko          2016   \n",
       "..               ...           ...   \n",
       "446        Jon Krohn          2023   \n",
       "447        Jon Krohn          2023   \n",
       "448        Jon Krohn          2023   \n",
       "449        Jon Krohn          2023   \n",
       "450        Jon Krohn          2023   \n",
       "\n",
       "                                    episode_split_text  \n",
       "0    This is episode number one with ex-chemical en...  \n",
       "1    This is session number two with machine learni...  \n",
       "2    This is episode number three with Nanophysics ...  \n",
       "3    This is episode four with business strategy ex...  \n",
       "4    This is episode number five with forensics inv...  \n",
       "..                                                 ...  \n",
       "446  This is episode number 679 with George Matthew...  \n",
       "447  This is episode number 680 with Allegra Alessi...  \n",
       "448  This is episode number 681 with Matt Harrison,...  \n",
       "449  This is episode number 682 with Mico Yuk, host...  \n",
       "450  This is episode number 683 with Dr. Matar Hall...  \n",
       "\n",
       "[451 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sds_ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391fedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove episode 202 and 546 with no text in episode_split_text\n",
    "#sds_processed = \n",
    "#sds_ds_processed[sds_ds_processed['episode_split_text'] == 'nan']#.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "826d945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text \n",
    "def processed_text(text):  \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation from the text\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Remove digits from the text\n",
    "    text = ''.join(char for char in text if not char.isdigit())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "876eba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['episode_split_text'] = sds_ds_processed['episode_split_text'].apply(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97fd0819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is episode number  with luke barousse of the eponymous data analytics youtube channel this episode is brought to you by iterative your mission control center for machine learningwelcome to the super data science podcast the most listenedto podcast in the data science industry each week we bring you inspiring people and ideas to help you build a successful career in data science im your host jon krohn thanks for joining me today and now lets make the complex simplewelcome back to the super data science podcast weve got the tremendous luke barousse on the show today luke is a fulltime youtuber creating highly educational but nevertheless often hilarious videos on data analytics for his enormous audience of over a quarter million subscribers previously luke worked as a lead data analyst and data engineer at basf the global chemical manufacturing giant before kicking off his career in industry he worked for seven years in the us navy on nuclear powered submarines and he holds a degree in mechanical engineering a graduate qualification in nuclear engineering and an mba in business analyticstodays episode is a terrifically interesting one because of the terrifically interesting guest so it might appeal to just about any listener but it will be particularly appealing to folks in the early stages of a data career such as those considering getting into data analytics in this episode luke details the musthave skills for entrylevel data analyst roles as demonstrated by an original data analysis project he carried out the data analyst skills mistakenly and erroneously pursued by many folks considering the career how his experience as a submariner prepared him well for a data analytics career his favorite tools for creating interactive data dashboards his favorite scraping libraries for collecting data from the web this skills people can amass now to be prepared for the data careers of the future and the benefits of crossfit beyond just the fitness improvements all right you ready for this especially engaging episode lets goluke barousse welcome to the super data science podcast i have wanted to have you on the show for ages ive been following your brilliant youtube channel for months and months and now to have you not in the flesh i guess but interacting with me on screen is such a thrill thanks for coming on luke how you doing where are you calling in from heck yeah doing well thank you so much for having me on first off when i first got into data analytics i found the super data science podcast and was just like oh my god theres such a wealth of knowledge so its awesome to come sort full circle and be on here so thank you wow im delighted to hear that and calling from arkansas in the united states sort of the middle of nowhere im near the walmart headquarters no affiliation but they have some mountain biking here i like to do mountain biking and so thats what im doing here the mountain biking isnt affiliated with the waltons in any way oh its very much affiliated with the waltons it is the mountain biking is theyve poured millions of dollars into building the trails around here the waltons are big to mountain biking really yeah no kidding its their way to basically build up the city to entice the vendors of walmart and people employees to move to bentonville arkansas its building up the trails because now you have this great little ecosystem and so people are like i want to go mountain bike there and they move here and they have something to do while working for walmart wow i had no idea i was saying that as a sarcastic joke yeah not sarcastic at all i appreciate there are millions of dollars they put into the trail and i get to ride for free amazing your youtube channel as ive already mentioned is incredible and lots of people think so at the time of recording you have over  subscribers by the time this episode is live im sure it will be many many thousands more your target market with your videos is primarily aspiring and entrylevel data analysts in the channel you creatively combine amazing different kinds of videos some of them are stepbystep instructions for doing things some of them are hot takes youve got engaging visuals and my favorites are the humorous skits sometimes you do them with other wellknown youtubers and i just get such a kick out of them i love them so luke you have an innovative channel ive never seen anything like it in the space where do you draw inspiration on for your next video for inspiration well it comes away with anybody but mainly i try to go back to whenever i first got into data science and more specifically data analytics i try to think of the questions and the issues that i had when i was first starting and what resources were not there because theres a lot of great youtube videos out there especially on tutorials i used to do a lot of tutorials and thats great its like hey i created tableau tutorial but when i first got into data science data analytics i didnt know what tableau was i didnt know what visualization softwares were what the need was at the time i was going to school for business and i knew i wanted to apply analytics and somebody came in and gave a presentation at the time on tableau and really helped open my eyes to it but that presentation thats not normal to understand what this software is used for its really hard to just get a concise understanding of what different tools are used to accomplish jobs so youre providing context youre kind of thinking if somebody came up to you and was like hey ive heard about this data analytics career where do i get started what are the different kinds of things you do those are the kinds of videos you think about youre going to provide some context for people yeah and with that probably a bad analogy sort of a gateway drug but basically make it as easy as possible for people to understand things they need to get into first for this field so that way they can decide hey maybe this isnt for me vice they dont even know data analytics and then they spend all this time learning tableau once again spend all this time learning tableau and then they start implementing and theyre like i dont really like this instead look at it holistically take a step back and provide a big picture look at these things so that way they can get more of this big picture view to figure out if they want to go into it and then what they need to actually focus on as you know theres so much you can focus on and so much you can look at i think you need somebody to just provide that basic understanding and big picture of whats happening there i love that whats the most gratifying outcome you get out of having this channel and these enormously popular videos yeah so i was actually just thinking about that the other day ive been doing this i guess two years now which i mean i guess isnt that long but man i can tell you usually ill finish up a video and then ill go to upload it and then ill upload it for the next morning and the night before i upload its like a kid whenever christmas is the next day and youre all excited or whatever thats how i get with these videos because im like i dont know what people are going to say i dont know but im looking forward to i just spent two three weeks making this video im pumped to release this thing and im excited sometimes i cant even sleep im ready to wake up the next morning and just check the comments and see what people are saying thats really what invigorates me is that cycle of getting to that point of releasing it and then seeing everybody seeing the benefit that it has yeah thats mainly just what gives me energy love it and then in particular your comedy skits where do those ideas come from theyre really innovative yeah well so i mean ill be completely honest i didnt originate those i mean theyre obviously on youtube but i follow this other youtuber called julie nolke and she has a famous youtube video its called explaining the pandemic to my past self basically the pandemics happening and you explain it to your past self before and it was sort of a skit talking to each other anyway its a funny little skit and im like oh she doesnt have any people shes just using herself and talking to herself im like i could do that but with data science and thats what i try to do a lot with my youtube videos just go about a different way of explaining concepts to peopleits actually pretty interesting i dont do the skits as much because sometimes they dont resonate as much but its very interesting i found that this was my data analyst versus data scientist one i received a lot of comments around one new people like hey i dont understand this but im looking forward to whenever i can understand it then ive had people that have been like hey ive been studying for six months now and now i can understand the jokes in this video this is awesome its funny because it can provide i guess a goal for people that are new to this field of maybe some inside jokes that they should be able to get yeah i get that i suspect that there are some listeners of the super data science podcast out there for example that are interested in the field theyre interested in data science and ai and sometimes we have episodes that get quite technical but they listen to those episodes anyway because theyre just kind of understanding the way that people use the language what are the trendy words even if you cant really dig onto the meaning of it just hearing those conversations happen i guess if you were in an office and there were some people who you were thinking about moving into the kind of role that they have and you kind of hear them talking over lunch that might be an interesting conversation to hear even if you cant totally understand everything but then as you explore the field more and more you get the inside jokes you get all the context yeah thats cool yeah i mean you got to fake it till you make it i like how people can use it for that and basically just like hey get exposed to these different topics and maybe in a fun way podcast way whatever over time i mean if youre exposing to it enough youre going to get it and that especially i think if you listen to a lot of episodes of this show or you watch a lot of your videos people will start to see oh sql that comes up all the time right i probably should focus pay attention to this one of your most popular videos outlines a learning roadmap to become a data analyst so sounds kind of the perfect video youre describing for providing the context to people do you think that a lot of people get lost or overwhelmed as they start getting into this data analyst career do you think its overwhelming looking at youre kind of describing earlier the situation where people might not know what tableau is and they look up what they need to know as a data analyst and all these videos come up about tableau and theres tableau tutorials and theyre like i dont even know what this is they press play and they dont understand whats going on and so its kind of overwhelming i guess that kind of situation must happen a lot with people right yeah i think its a lot i actually did a bunch of interviews with my subscribers and for those that are entering the field of data analytics and what their biggest problem is in it there was two main things one of them was being overwhelmed and the other one was not really having a lot of social interaction basically because were in the pandemic not in the pandemic were through the pandemic but during the pandemic you didnt really have a lot of people and a lot of interactionbut the overwhelming aspect yeah just exactly to your point theres so many different things where do you focus you go to these job descriptions and you theyre like you want to be an inspiring data analyst all of them are saying different skills that you need to notice youre like what do i need to learn and so that was really the thought of the process of that video was to basically narrow down what you should focus on because i had a lot of problems i learned tools that i think ive wasted my time on and i was like i wish i would have just had somebody who had been like hey focus on these core tools do you have any examples of tools that you wasted your time on off the cuff yeah i mean im going to bad talk it right now but microsoft access i cant stand youll have to bleep the cuss words that are going to come out of my mouth and i trash talk it anytime i cannot stand that program i mean its an outdated program microsofts trying to phase it out but companies are still using it and they cant get rid of it im like dont learn this anyway i need to stop right now im so sorrybut ive also learned i dont even want to give a shout out to the name of this company because they upset me so bad an automation tool to extract data and their whole premise was basically locking businesses in to paying a subscription fee to use this tool to web scrape data i found out later that i could just use python even more easily because it was supposed to be a low to no code solution and i actually was having to use code and so it was like oh man i wish i had just known python i wouldve used that yeah those two situations my bloods boiling right now would come to mind and im just like im so glad i wish that somebody wouldve been like hey focus on these things yeah what python libraries do you like to use for web scraping beautiful soup yeah beautiful soup selenium pros and cons between those two i couldnt give you that i can just say i like selenium mainly because usually im doing clicking or something or need to scroll on the page so i need that sort of browser atmosphere and yeah im just used to it so i just stick with it this episode is brought to you by iterative the opensource company behind dvc one of the most popular data and experiment versioning solutions out there iterative is on a mission to offer machine learning teams the best developer experience with solutions across model registry experimentation cicd training automation and the first unstructured data catalog in query language built for machine learning find out more at wwwiterativeai thats wwwiterativeaicool well were going to talk more about your favorite tools later in the episode but lets kind of get back to where we were in the conversation about people getting started in data analytics what is the most significant assumption you think aspiring data analysts make when planning out their careers i guess a kind of interesting twist on that question what are the assumptions they make that are misplaced about a data analytics career alex the analyst i think was talking about this so i dont want to steal his idea but im basically going to steal his idea i saw he made a linkedin post about it recently i think people think that you have to go especially for data analytics data analysts they want an employer needs basic things done with data analytics whether its excel sql python r whatever just basic tasks done to analyze data i think one of his recent linkedin posts he was talking about people present this portfolio for an entry level data analyst and it has all this machine learning algorithms and advanced methods of using gradient descent and stuff like thati mean thats great but i need you to analyze this budget that i have right here and maybe provide some deficiencies i dont think were going to use a machine learning algorithm on it i think people one they get a little too overwhelmed and they can think that they need to know stuff thats way more advanced than they actually need to and that they just need to focus on the basics and understand that and display those skills in order to land an entry level job cool yeah thats pretty similar we recently had shashank kalanithi on the show you probably know that love me some shank yeah so he was in episode number  and that was something that he said too he was like for data analytics for an entry level data analyst role i might not be able to remember totally off the cuff here but he was saying things like youve got to have some understanding of bi tools maybe something like tableau he was like you can learn tableau in a weekend and then spend a couple of weeks learning simple sql queries be able to do some relatively advanced excel stuff like vlookup and youre set on the job then you can maybe learn some python thatll help you automate some things but he was like you can be prepared for a data analyst career in a couple of months if you focus on it full time yes so true if you could just focus i like how you talked about those tools specifically from him because thats exactly what i would recommend my first job i just knew mainly excel but then i got into it got the job and then i started to learn power bi and implementing power bi in my job then my next job i started implementing sql and also had to do tableau so i started learning that you can sort of snowball and learn more tools as you progress in your career but you have to start first at those core tools to even get the job nice tell us about power bi thats also a microsoft tool right so you mentioned microsoft access earlier i dont want to get you fuming again it sounds like microsoft can build tools that are useful i mean obviously excel famous probably the most widely used data analysis tool on the planet whats the difference why would somebody start using power bi instead of excel what does it do differently i think so the ability to actually build succinct dashboards that people cant mess with when i first started i made one of my first roles as a data analyst i made this dashboard in excel and you can make dashboards in excel but one theyre sort of clunky two but then how do you share this excel dashboard you have to send the file and then what happens whenever you have to update it you have to send the new file i guess you could host it you could host it on excel online but then you can mess with your cells you could mess with stuff thats why i like power bi power bi is a dashboard solution provides visualizations and i can use this within microsoft this power bi service make this dashboard that people cant mess with and well that they can play with but they cant you can give them knobs and drop down menus slicers yeah clicks whatever they want to do i can set up this controlled environment for them to go in use the data and then this thing this dashboard can connect to a variety of sources which may be excel files it may be a sql database or whatever thats why im like oh im really bullish on power bi i just love it i mean tableaus just as good i dont think either ones different it depends on what the companys using but im a fan of power bi and ive made a couple of courses on it so im just like yeah im very biased towards it super cool we havent talked about power bi very much in the show at all so im delighted that you can provide us with a bit of an introduction to it thanks yeah and i guess the only other thing that i want to add to that is the one problem that i have with power bi still so tableaus great for visualization softwares you usually have tableau and power bi tableau has this thing called tableau public and you can go and share your dashboards very easily on tableau public people can go in and look at your dashboard play with whatever data its connected to and its really great microsoft power bi doesnt have anything public facing like that so if youre within an organization you can use your organizations microsoft suite and share it internally but if i wanted to build a dashboard me without an organization and to be able to share that dashboard i cant really do that easy and so thats what one major drawback and i want to just capture on nice super cool all right so weve talked about some assumptions that people might make early in their data analyst career that are maybe misled how about you luke what are the biggest mistakes that you made early on in your data analyst career that you wish you could go back and do differently yeah so i think that one of the biggest mistakes besides not picking the right tools is then once getting into my job itself or any project if you will i think one of the biggest problems i had was focusing too much energy on a specific task or a specific project before getting input from other stakeholders i guess an example of that was in my first job im building this product and i built out this talked about this excel dashboard built out this excel dashboard and i was like oh were going to do so many different metrics on it were going to be doing standard deviation variance all these different things i spent weeks on this thing and then i give it to my boss and it did have the information that he wanted but unfortunately there was a lot of information and a lot of time that i spent adding other things that wasnt any value add to the project and didnt provide any insightsi wish i could tell you that i only did that mistake once but i did that in following projectsthats why i want to bring it up now because it happened more than once and its mainly hey you work on something get insights get quick feedback even if its not a final product its always good to have those stakeholders or whoever youre building this for to look at it and maybe provide that feedback to make sure youre going on the right track so youre not going to waste unnecessary time i did learn some more skills while doing it but i didnt get to where i needed to as efficiently as i could have nice so get feedback early and often is a tip that you wish you could give to your past self just like the woman that inspired you with that covid video talking to her past self right exactly in addition to your data analytics career you have had some tangential entrepreneurial undertaking in  you founded a lifestyle startup called macrofit to improve health through meal prep this is something that resonates deeply with me i am a big meal prep subscriber i subscribe to two different meal prep services i guess i might as well mention them on air so that people are aware if thats something youre thinking of to me this is a really obvious thingbefore we even started recording this episode luke and i talked for about half an hour about crossfit he and i have very much been drinking the koolaid i dont think its an unfounded drinking of koolaid if you are interested in forging elite fitness theres no easier way than being disciplined about going to crossfit its a key thing its easy if youre disciplined about doing itthe thing that makes it so easy is that you have a community that propels you when you know that youre going to be doing that workout against someone whos maybe been doing about as long as you they come to the same class time as you you want to beat them a bitbut you also want to celebrate your progress with other people you want to cheer on other peoples progress and so just having this community and its also been an amazing place for me personally to meet tons of great professional connections because unlike a lot of gyms out there a lot of other noncrossfit gym experiences ive had people put their headphones in they just kind of watch tvs that are on as they sit at a machinebut when you go to a crossfit gym a lot of crossfit gyms youre not even allowed to wear headphones you certainly cant in class time youve got to be paying attention everyones in sync you get to meet a lot of people you show up at the same class time every day youre going to make a lot of friends you meet a lot of professional connections its just such a wonderful community and then once you start doing that for a little bit youll start to realize that people in the crossfit community its not just about the fitness its about so many other aspects of your life that support fitnessgetting a good nights sleep not drinking too much stretching and things like diet diet is super important to being able to grow muscles to be able to expand your cardiovascular capacity to have energy throughout the day especially if you are working out in part of your day so super super important to have great nutritionfor me its been a no brainer for many years that the easiest way for me to get the macronutrients that i need so theres these kinds of rules of thumb you should eat as many grams of protein per day as you have pounds of body weight in order to sustain or grow muscle massthen depending on how much youre working out youre also going to need carbs sometimes people will be like keto its hard to be a serious athlete and be keto because you need carbs as fuel for your workouts depending on how much youre working out and you might want to work with a nutritionist or a crossfit coach or somebody to give you some guidelines on how many grams of carbs fat and protein you might want to get in a day but you can do your own meal prepone of the most time consuming ways of getting all your macros is to be weighing all your food maybe you spend your sunday doing meal prep for the week some people even theyll get groups together where youre like okay every fourth week youve got to do meal prep for four people including yourself so people will do things like that for me i was like theres meal delivery services they tell you exactly the macros on each meal i get this preweighed for three meals a day six days a week ive got preweighed meal containers that come from two services i use these are both usbased services but territory and megafit meals territory has a bit more vegetables so i like that and theyre really really yummy i might check that out yeah territorys good for that but territory also there tends to be a bit more fat in the meals its hard for me to hit my lower fat targets on the macro plan that im on theres this other company megafit meals which the meals arent as yummy and they definitely dont have enough vegetables but in terms of hitting your macros they have tons of meals with five grams of fat  grams of carbs  grams of protein so a few meals like that in the day and im off to the races with hitting my macro targets for the day all of that lead up was to say that you also luke created macrofit to improve health through meal prep so could you elaborate on this particular business idea what was it about what motivated you and what did you learn yeah so actually this has a lot of synergies i think with your first point about community with crossfit sort of like were doing here in data science you want to have likeminded people to understand what everybody else is doing for data science where you should be going and how to improve your career same thing with i think crossfit and you have a community to know where to go that was one of the problems that i noticed was that this meal prep and specifically this macros its very difficult macronutrients like you explained this protein carbs fat you have certain amounts that you need to do or you want to hit and they all make up how many calories you get in a day if youre below or above your certain calorie target for the day you can either gain or lose weightthese are numbers that im talking about grams of these macronutrients or numbers of calories these are numbers this is data so im like i could turn this into a data science project like what youre talking about oh i went over fats because i have too much fats in this one meal im like okay i want to make a solution that can basically tell me what i need to eat i know i want to eat three meals a day tell me with each meal one the macronutrients i need to hit and then also maybe make some recommendations of what i need to eat in order to hit those macro nutrientsthats what my goal was i built this in microsoft excel which started as a project in college with a bunch of other people and then i took it further after that class i want to make sure i give credit to those people i worked with initially in my school project we actually used macros like excel macros microsoft excel macro so we called the program at the time macros for macros because we used excel macros one of the girls in my group came up with that title im like oh yeah we got to use thisanyway thats all we really did we built this excel sheet where it was like hey you know what your current weight is what weight you want to get to so you could calculate you do the calculation in excel of how many calories you need to do in order to lose this weight over how many weeksthen from there we broke down the calories further with excel into different macronutrients and then i downloaded i went to the usda or whatever the food place is that gave the data for foods and their caloric and their macronutrients i got all that data and from there we built basically a simple little algorithm that basically would tell you okay this meal i need to eat one cup of rice you need to have five ounces of steak and you have a cup of veggies and maybe some sort of fat like a half avocadoit would tell you every single meal what you needed to do that was the data science project it was so crazy because i got to use excel use this data in sort of a different way i wasnt doing analytics but it was using data and calculating what you needed to do that got me into after the project was done in that i wanted to i was like i can make this into an app that once again gets into tools that i shouldnt have learned as a data analyst that i did but i got into learning a web framework with python into learning django yeah i dont know why i pronounced the d django i was like is that how you pronounce django no its not i mispronounced in one of my videos whats that oh really you mispronounced it in your video i mispronounced it in one of my videos i was like oh django and everybody was like this isnt the quentin tarantino movie its django and im like oh okay but during the pandemic or whenever when you dont have people around you to talk to you say things that you dont know because youre not talking to people yeah what do you think about the super data science podcast every episode i strive to create the best possible experience for you and id love to hear how im doing at that for a limited time we have a survey up at superdatasciencecomsurvey where you can provide me with your invaluable feedback on what you enjoy most about the show and critically about what we could be doing differently what i could be improving for you the survey will only take a few minutes of your time but it could have a big impact on how i shape the show for you for years to come so nows your chance the surveys available at superdatasciencecomsurvey thats superdatasciencecomsurveyone big one for me for a long time was the word awry yeah i didnt map saying i would say awry verbally to people i didnt know how to spell it im talking until my late twenties i didnt really ever think about how youd spell awry then whenever i read it in a book i was like ori and i kind of knew what it meant i didnt really realize that theyre the same word so i get that but in terms of a podcast you actually mispronouncing django as django helps people find that tool although now youre not even recommending it but thats one of the tools i was learning because i was like im gonna build this app and thats not what data analysts should be doing thats the data analyst thats submitting their machine learning algorithms to get this its not something i should have done i wasted a lot of time and effort trying to build this app that eventually just like i was like django is a really hard and really difficult web framework and i could have been spending my time on other things anyway so i eventually abandoned it but during that time i was trying to promote this and i was using instagram and also youtube to try to promote this application thats really how i got into social media was through this data science trying to promote this application via social medias oh wow that ties in really nicely to my motto to my favorite saying luke which is its a latin phrase its qui audet adipiscitur i did a whole episode on it episode number  it translates into english as who dares wins where by trying your project by trying the macrofit project that got you into doing social media stuff that has now turned into this incredible youtuber career that you have where youre inspiring hundreds of thousands of people millions of people this whole idea is that when you do something when you take action especially if that action is audacious then even if you dont achieve the initial outcome that you envisioned which is creating this nutrition app great things will come from it who dares wins thats crazy that sounds interesting yeah i think that relates because it was just so crazy because it was just like yeah the goal was to build this app and then i started i was from an instagram i was taking pictures of food as silly as i might sound but it was actually sort of fun and i enjoyed the social media aspect and im like i actually like this social media aspect better than and sharing these data insights but i like this better than actually trying to build this django app so yeah thats crazy yeah cool career transition luke it isnt the only super cool career transition that youve had due to taking on audacious challenges its interesting because that who dares win quote was popularized in recent centuries by the british royal air force one of the most interesting things about you is that for seven years you were a submariner in the navy and in order to run a nuclear submarine you were trained in nuclear engineering thats wild to me thats such a cool specialization to have about the world wow can you tell us a bit about that experience and did nuclear physics help in your data analyst career later yeah so i served seven years in the united states navy specifically in the submarine force which all of our submarines in the united states navy use nuclear power yeah well it was a crazy time just so much time spent almost close to two years of my life underneath water its just like oh my twenties completely gone because or not i wouldnt say completely gone it was a great time great learning experience but so much time away from family and friends out to sea running a submarine going places theres parts that i miss especially the camaraderie and everything like that but then theres parts of im looking out of a window right now i actually have a window and i can go outside later whereas i would go on this submarine i would go underwater for three four months at a time oh my god yeah we just go for three or four months at a time the only thing were really limited by because its nuclear power we can make our own power we can make our own water were limited by food thats whenever we have to come back to land actually our submarine was not one of the bigger ones and so youd have to there wasnt a lot of food storage and so youd actually have cans of food on the ground and we put plywood on top of it and then you would eat those cans of food and then you can remove the plywood but sometimes some of the areas that you walk through the submarine it was even shorter than usual because you had these cans of food that you had to walk on wow that is so interesting i had no idea i kind of thought it would be three or four days at most i thought and then my liberal estimate once you started saying three to four i thought at most you would say weeks three to four months thats wild how do you make the water you take salt water and then you use nuclear energy to desalinate it yeah so they have two different types i hope this isnt classified i dont think it is its pretty open knowledge i think you can either boil the water ill give general examples you can either boil the water theyre called an evaporator and so you use this hot water and boil the water and it cleans the water or you use a reverse osmosis unit which is more just powered through electricity to shock it and get out the clean water we had the evaporator option on mine and you can only make so much and you can only make it so fast so water conservation they call it they have submarine showers youre supposed to get in bathe real quick wet yourself shower down turn off the water soap up and then rinse off and then get out youre not supposed to waste water i mean thats amazing that theres enough to be showering at all thats wild yeah well i mean the submarines are pretty theyre pretty oh i mean theyre such fastening things i mean like a football field long and just tanks go lower with freshwater tanks sanitary tanks to store your waste ballast tanks so large tanks to ballast the ship up and down and its just but i guess this sort of relates to your question of how does this relate to my analytical careera lot of that stuff i didnt realize at the time but were dealing with data at real time on a submarine you have to analyze this data all the time i had to work for a little bit in the engineering power plant so i would run the nuclear reactor me and a team of  other individuals and we have to look at all these different gauges monitor pressure inside of the reactor temperature inside the reactor look at trends you have to plot it theres a lot of look at chemistry understand the trends that are going on with the chemistry inside of itanyway theres a lot of numbers and so theres a lot of understanding behind whats happening with the data and where its going it was just sort of second nature whenever i transitioned to a data analyst coming from that role because its like im surrounded by numbers all the time just now i have i dont want to do talk bad about the government but now i have just better tools to analyze the data now because we were doing paper logs and the navy has improved the paper logs recording these numbers for all these things and weve improved in that way but now i have advanced tools that i can analyze the stuff and do some more trend analysis so yeah theres i guess pros and cons super cool really delighted that we had that question to ask you because wow i just learned some amazing things another project another kind of career twist that youre working on right now though firmly rooted in now your data analysis and data science and even your web scraping experience specifically you alluded before we started recording to me about a job data collection project do you want to tell us a bit about that yeah so i started that last year well so i have all these im a data analyst obviously and you have all these people asking me like hey what are the top skills what skills should i be focusing on and i could tell them i could tell them what it is what i think it is but whats the data say im like i need to get the data so you have stack overflow if anybody doesnt know stack overflow is like a coding website and it collects all these coding questions you can go on and see it anyway they provide a survey every year that will tell people what are the most popular languages what are the most popular sql databases its actual data of whats important but its more focused for developers and web developers i mean there is a sprinkle of data scientists and stuff in there but mostly its i think good developmentanyway they have data its like solid data you can look at it theres nothing like that for data analysts its like where do you go to collect that how do you know what jobs require what skills so i was like i need to collect this for job postingsso last year i built this python bot because i didnt want to use that one solution that i talked about from work that was basically a money ponzi scheme that i feel i used python instead to go in to linkedin which not really allowed but did it anyway go into linkedin and scrape job postings of data science roles data analysts data scientists data engineers scrape all that data and then from there pick out those courses just go through and pick out what skills and then from there aggregate it to find out what are the most popular skillsi did that for three months last year running this bot every single day i got a ton of job postings a ton of data and actually was able to go in and list like excel sql being the top two skills and then from there the visualization tools and programming languages so power bi tableau python and r theyre all about equal those are basically the top eight skills that i found as data analystssorry funny side note real quick because i was thinking about that today i shared this and excel was the number one from the data because i finally had this and somebody multiple people commented excels not a core skill of data analysts and im like im literally showing you what the data says dont be mad at me im showing you what the data is the data says excel is the most popular skill i dont like it i dont like it either i would much rather it be python but dont be mad at meanyway i ran into a lot of issues with linkedin trying to block me doing the web scraping so i sort of halted it after a while and im trying to get back into it im working with the company right now to try and get this data and actually start collecting it on a daily basis what i would love to do to continue this project is continue to collect it over time and you know how they have those races of programming languages over time whats the most popular programming language you could watch it oh python in  it was nothing but now its like oh its high up there thats like the goal collect this on a daily basis just indefinitely and start mapping out what is the most popular skills so people know as they can go and they can focus on the top skills for data analysts i know that you dont have this developed yet but just based on your experience do you have any predictions as to what tools someone whos interested in getting into data analysis today should be learning what are the tools of the future that people should be learning today yeah its not changing very quick for data analysts unfortunately i dont know if you feel the same but i feel like were sort of slow moving in regards to that in regards to technology i mean yeah specifically for data analytics i feel im still pretty excel and sql i feel like first two main ones and then a programming language and a visualization tool and thats what i would stick with what about are there anything python libraries or anything that youre really excited about right now that you think people should get into or a tool or approach that youve recently discovered that youre like oh man i really need to dig into that and learn more about it yeah im really yes i have an answer for that and it really relates to data engineering because as a data analyst thats the biggest problem that ive found this problem right here i need this data of job postings and its not easy to get its a data engineering problem a lot of my subscribers are getting theyll get this google data analytics certificate and theyll actually go into data engineering because its such strong demand so tools around data engineering i think are invaluable to learn whether that be around how to manage databases like snowflake databricks or how to do orchestration i like airflow which is a library of python i really like that language and think thats so cool those type of things that would be sort the fun things that i want to learn but i would not unfortunately i wouldnt be like youre an entry level analyst you should go learn airflow i would be like you need to focus on excel and sql but if you want to continue its a great answer though for the question that i asked of what can they be doing to prepare for the future i think this is something as data sets continue to get exponentially larger every year increasingly data analysts to be effective are going to have to be able to do some of their data engineering otherwise a company would have to hire a separate data engineer to be providing data to one or more data analysts which is an added expense if they could find somebody that could do both itd be ideal actually shashank in his episode also talked about how he thinks that the best data engineers are people who used to be data analysts which is a transition that hes made he says that when you know what the downstream user is going to need when you appreciate their concerns youre going to be better at extracting the data and providing those data to them i think once again that gateway drug data analysts are like that once youre that entry level data analyst you sort of peek into data engineers data scientists or maybe a more advanced data analyst and you could then choose where you feel like you should focus more nice well then i think you actually have kind of answered the question that we had prior to you being on the show i posted on linkedin and twitter and asked if people had questions for you mattias baldino who is a bi analyst at a company called brain technology he wants to know what you expect of your own future maybe you can expand a little you can try to look in your crystal ball a little further in your career but he says does luke see himself moving to a leading role or moving to a role more focused in data science he says youre always giving advice to aspiring or new data analysts but hed love to know your own thoughts about how youd like to move forward with your data analyst career we kind of already got a sense that data engineering is in that vision but i dont know if you have anything else youd like to add maybe looking further beyond where would you like to be in a decade yeah so i love data science but as we talked about already i love the social media aspect and being able to bridge people thats really i dont look to be a ceo or a lead data analyst or lead data scientist i want to learn about new technologies and i want to be able to share that with others and i think social media provides that platform for me to grow mentally and then also share it thats where i want to grow is through youtube and like you talked about at the beginning im focusing on entry level analysts but id love to expand this to help more than just entry level data analysts in the future i love that answer and im so excited thats your answer because it means that i get to continue to enjoy more and more luke barousse youtube content in the future i love it all right luke so those are all the questions ive got for you before we end the episode i always ask for a book recommendation you got one for us yeah so i guess top book actually i made a video on it but for our data analysts and also data scientists and maybe data engineers i would recommend storytelling with data with cole knaflic she was on the super data science podcast and its such an insightful book into how to communicate with data actually funny enough i read this years ago and this book was actually a huge motivator not only for my prior social media experience but this was a huge motivator for me to try to tell stories with my youtube channel with data anyway besides that i think its must read and its so easy to read and it just provides such immense amounts of detail and insights that im like i cant recommend it enough that is a cool recommendation cole nussbaumer knaflic she was on episode number  and somebody i cant remember off the top of my head right now so some listener out there was like you bonehead jon krohn i know who it was in a very recent episode somebody else recommended that same book oh okay yeah so really popular recommendation i wonder if shashank did because he also interviewed her on his youtube channel i think and she has a new book out too yeah theres a good chance that it was shashank i cant remember off the top of my head but yeah ive had this conversation recently so yeah super cool recommendation and that was a very strong recommendation must read from luke barousse and also recommended by someone else possibly shashank check it out listeners it was episode number  of the super data science podcast that she was on sounds like maybe we should be having her on again because of her next book yeah probably cool well theres a thought all right luke well obviously people know that if they want to keep tracking your career after this your youtube channel is probably the primary place to do it any other places that people should be following you youtube mainly thats where i want to focus most of my effort i need to beef up my crew and mainly i need an editor but that and then linkedin is the main two places im also on instagram and tiktok nice all right well sure to include your social media links in the show notes for this episode luke i had such a great time chatting with you today its been such a fun episode thank you for coming on its been an honor to meet you and i look forward to catching up with you again sometime soon heck yeah thank you for having me on wish we could have talked more about crossfit but im glad we talked about data wow i loved chatting with luke today what a guy in todays episode luke filled us in on how selenium and beautiful soup are his favorite web scraping libraries how data analysts dont need machine learning projects in their portfolio to get an entry level role instead they should focus on the essentials namely excel sql a dashboarding tool like tableau or power bi and maybe a programming language like python or r he told us how his biggest mistake in his early career was not getting regular enough feedback from stakeholders on consulting projects he also told us how his experience with realtime submarine data was foundational for his data analyst career while trying to build his macrofit data product was instrumental in him becoming a fulltime content creator he told us how familiarity with data engineering tools like snowflake databricks and airflow will prepare you for the data jobs of the futureas always you can get all the show notes including the transcript for this episode the video recording any materials mentioned on the show the urls for lukes social media profiles as well as my own social media profiles at superdatasciencecom every single episode i strive to create the best possible experience for you and id love to hear how im doing at that for a limited time we have a survey up at superdatasciencecomsurvey where you can provide me with your invaluable feedback on the show again our quick survey is available at superdatasciencecomsurveythanks to my colleagues at nebula for supporting me while i create content like this super data science episode for you and thanks of course to ivana mario natalie serge sylvia zara and kiro on the super data science team for producing another magnificent episode for us today for enabling this super team to create this free podcast for you we are deeply grateful to our sponsors please consider supporting the show by checking out our sponsors links which you can find in the show notes if you yourself are interested in sponsoring an episode you can get the details on how by making your way to jonkrohncompodcastlast but not least thanks to you for listening all the way to the end of the show until next time my friend keep on rocking it out there and im looking forward to enjoying another round of the super data science podcast with you very soon'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sds_ds_processed['episode_split_text'].loc[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18c02451",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['episode_keywords'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4892f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate 100 keywords per episode using Yake library to store keywords in one column as list\n",
    "def yake_keyword_extractor(doc):\n",
    "    global sds_ds_processed\n",
    "    \n",
    "    for i, episode_text in doc.iteritems():\n",
    "        max_ngram_size = 3\n",
    "        deduplication_threshold = 0.3\n",
    "        windowSize = 1\n",
    "        numOfKeywords = 100\n",
    "        kw_extractor = yake.KeywordExtractor(n = max_ngram_size, dedupLim = deduplication_threshold , windowsSize = windowSize, top = numOfKeywords)\n",
    "        keywords = kw_extractor.extract_keywords(episode_text)\n",
    "        sds_ds_processed.at[i, 'episode_keywords'] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yake_keyword_extractor(sds_ds_processed['episode_split_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_extractor(row):\n",
    "    words = [t[0].replace(\" \", \"_\") for t in row]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed['keywords'] = sds_ds_processed['episode_keywords'].apply(tuple_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_processed.to_csv('../data/sds_ds_keywords.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c4323",
   "metadata": {},
   "source": [
    "## Generating keywords for non-Data Science Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed = pd.DataFrame(sds_non_data.groupby(['episode_number', 'episode_name', 'length_episode', 'context_episode', 'guest_name', 'host_episode', 'episode_year'])['episode_split_text'].agg(lambda x: ' '.join(str(i) for i in x))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed['episode_split_text'] = sds_nds_processed['episode_split_text'].apply(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bbf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed['episode_keywords'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06541a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate 100 keywords per episode using Yake library to store keywords in one column as list\n",
    "def yake_keyword_extractor(doc):\n",
    "    global sds_nds_processed\n",
    "    \n",
    "    for i, episode_text in doc.iteritems():\n",
    "        max_ngram_size = 3\n",
    "        deduplication_threshold = 0.3\n",
    "        windowSize = 1\n",
    "        numOfKeywords = 100\n",
    "        kw_extractor = yake.KeywordExtractor(n = max_ngram_size, dedupLim = deduplication_threshold , windowsSize = windowSize, top = numOfKeywords)\n",
    "        keywords = kw_extractor.extract_keywords(episode_text)\n",
    "        sds_nds_processed.at[i, 'episode_keywords'] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "yake_keyword_extractor(sds_nds_processed['episode_split_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7427f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed['keywords'] = sds_nds_processed['episode_keywords'].apply(tuple_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_processed.to_csv('../data/sds_nds_keywords.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62198e54",
   "metadata": {},
   "source": [
    "### Count Vectorizer and Class-Based Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b326cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_keywords = pd.read_csv('../data/sds_ds_keywords.csv')\n",
    "\n",
    "keywords_per_class = sds_ds_keywords.groupby('context_episode')['keywords'].apply(' '.join).reset_index()\n",
    "\n",
    "# Rename the columns to match the original code\n",
    "keywords_per_class = keywords_per_class.rename(columns={'context_episode': 'Class', 'keywords': 'Document'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eebfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTFIDFVectorizer(TfidfTransformer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CTFIDFVectorizer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def fit(self, X: sp.csr_matrix, n_samples: int):\n",
    "        \"\"\"Learn the idf vector (global term weights) \"\"\"\n",
    "        _, n_features = X.shape\n",
    "        df = np.squeeze(np.asarray(X.sum(axis=0)))\n",
    "        idf = np.log(n_samples / df)\n",
    "        self._idf_diag = sp.diags(idf, offsets=0,\n",
    "                                  shape=(n_features, n_features),\n",
    "                                  format='csr',\n",
    "                                  dtype=np.float64)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: sp.csr_matrix) -> sp.csr_matrix:\n",
    "        \"\"\"Transform a count-based matrix to c-TF-IDF \"\"\"\n",
    "        X = X * self._idf_diag\n",
    "        X = normalize(X, axis=1, norm='l1', copy=False)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create c-TF-IDF\n",
    "count = CountVectorizer().fit_transform(keywords_per_class.Document)\n",
    "ctfidf = CTFIDFVectorizer().fit_transform(count, n_samples=len(keywords_per_class)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag of words using count vectorizer\n",
    "count_vectorizer = CountVectorizer().fit(keywords_per_class.Document)\n",
    "count = count_vectorizer.transform(keywords_per_class.Document)\n",
    "words = count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d1fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc9b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9716a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = pd.DataFrame({\n",
    "    'words': count_vectorizer.get_feature_names_out(),\n",
    "    'frequency': np.array(count.sum(axis = 0)).flatten()\n",
    "})\n",
    "word_counts.sort_values('frequency', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts[word_counts['frequency'] < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class based tfidf\n",
    "ctfidf = CTFIDFVectorizer().fit_transform(count, n_samples=len(keywords_per_class)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156230e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique words in each class\n",
    "unique_words_per_class = []\n",
    "for i, class_docs in enumerate(keywords_per_class.Document):\n",
    "    tfidf_scores = ctfidf[i]\n",
    "    unique_word_indices = np.argsort(tfidf_scores)[-20:][::-1]  # index\n",
    "    unique_words = [words[idx] for idx in unique_word_indices]  # feature name\n",
    "    unique_words_per_class.append(unique_words)\n",
    "\n",
    "class_unique_20 = pd.DataFrame({'Class': keywords_per_class.Class, 'unique_Words': unique_words_per_class})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1089ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_20['unique_Words'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cccd274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_20['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603207e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_unique_20.to_csv('../data/keywords_u20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed37514",
   "metadata": {},
   "source": [
    "### Generate word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68500d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_keywords = ' '.join(keywords_per_class.Document)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_keywords)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = ' '.join(map(str, class_unique_20.unique_Words))\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_keywords)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate 100 keywords per episode using Yake library if keywords need to be separate columns\n",
    "\n",
    "def yake_keyword_extractor(doc):\n",
    "    global sds_processed\n",
    "\n",
    "    for i, episode_text in doc.iteritems():\n",
    "        max_ngram_size = 3\n",
    "        deduplication_threshold = 0.3\n",
    "        windowSize = 1\n",
    "        numOfKeywords = 100\n",
    "        kw_extractor = yake.KeywordExtractor(n=max_ngram_size, dedupLim=deduplication_threshold, windowsSize=windowSize, top=numOfKeywords)\n",
    "        keywords = kw_extractor.extract_keywords(episode_text)\n",
    "\n",
    "        # Create separate columns for each keyword\n",
    "        for j, keyword in enumerate(keywords):\n",
    "            column_name = f'episode_keyword{j+1}'\n",
    "            sds_processed.at[i, column_name] = keyword[0]  # Store the keyword value in the respective column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_extractor(doc):\n",
    "    kw_model = KeyBERT()\n",
    "    global sds_processed\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    \n",
    "    for i, episode_text in doc.iteritems():\n",
    "        keywords = kw_model.extract_keywords(episode_text, keyphrase_ngram_range=(1, 1), stop_words = stopwords, top_n = 100, use_mmr=True, diversity=0.7)\n",
    "        sds_processed.at[i, 'episode_keywords'] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffee6d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://maartengr.github.io/BERTopic/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
