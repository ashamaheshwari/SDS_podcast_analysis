{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78574cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a2653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds = pd.read_csv('../data/sds_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b710d4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_name</th>\n",
       "      <th>length_episode</th>\n",
       "      <th>context_episode</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>guest_info</th>\n",
       "      <th>text_episode</th>\n",
       "      <th>episode_number</th>\n",
       "      <th>episode_date</th>\n",
       "      <th>episode_year</th>\n",
       "      <th>episode_day</th>\n",
       "      <th>host_episode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ruben Kogel on Self-Serve Analytics, R vs Pyt...</td>\n",
       "      <td>42</td>\n",
       "      <td>Business Data Science Database</td>\n",
       "      <td>Ruben Kogel</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Kirill: This is episode number one with ex-che...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sep 10, 2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning, Recommender Systems and the...</td>\n",
       "      <td>51</td>\n",
       "      <td>Machine Learning Data Science</td>\n",
       "      <td>Hadelin de Ponteves</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Kirill: This is session number two with machin...</td>\n",
       "      <td>2</td>\n",
       "      <td>Sep 14, 2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Defining the Data Problem, Data Science in Ma...</td>\n",
       "      <td>53</td>\n",
       "      <td>Machine Learning R Programming Data Science</td>\n",
       "      <td>Dr. Wilson Pok</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Kirill:\\tThis is episode number three with Nan...</td>\n",
       "      <td>3</td>\n",
       "      <td>Sep 25, 2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data and Strategy, three Pillars of Research ...</td>\n",
       "      <td>60</td>\n",
       "      <td>Business Data Science</td>\n",
       "      <td>Brendan Hogan</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Kirill: This is episode four with business str...</td>\n",
       "      <td>4</td>\n",
       "      <td>Oct 02, 2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Computer Forensics, Fraud Analytics and knowi...</td>\n",
       "      <td>63</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Dmitry Korneev</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Kirill:\\tThis is episode number five with fore...</td>\n",
       "      <td>5</td>\n",
       "      <td>Oct 09, 2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Kirill Eremenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>The A.I. and Machine Learning Landscape, with...</td>\n",
       "      <td>94</td>\n",
       "      <td>Business Data Science Artificial Intelligence</td>\n",
       "      <td>George Mathew</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Jon Krohn:\\t00:00:00This is episode number 679...</td>\n",
       "      <td>679</td>\n",
       "      <td>May 16, 2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Jon Krohn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>Automating Industrial Machines with Data Scie...</td>\n",
       "      <td>30</td>\n",
       "      <td>Business Data Science</td>\n",
       "      <td>Allegra Alessi</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Jon Krohn:\\t00:06\\tThis is episode number 680 ...</td>\n",
       "      <td>680</td>\n",
       "      <td>May 19, 2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Jon Krohn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>72</td>\n",
       "      <td>Machine Learning Data Science Python</td>\n",
       "      <td>Matt Harrison</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Jon Krohn:\\t00:00:00\\tThis is episode number 6...</td>\n",
       "      <td>681</td>\n",
       "      <td>May 23, 2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Jon Krohn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>Business Intelligence Tools, with Mico Yuk</td>\n",
       "      <td>28</td>\n",
       "      <td>Business Data Science</td>\n",
       "      <td>Mico Yuk</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Jon Krohn:\\t00:05\\tThis is episode number 682 ...</td>\n",
       "      <td>682</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Jon Krohn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>Contextual A.I. for Adapting to Adversaries, ...</td>\n",
       "      <td>81</td>\n",
       "      <td>Data Science Artificial Intelligence</td>\n",
       "      <td>Matar Haller</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Jon Krohn:\\t00:00:05\\tThis is episode number 6...</td>\n",
       "      <td>683</td>\n",
       "      <td>May 30, 2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Jon Krohn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>682 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          episode_name  length_episode  \\\n",
       "0     Ruben Kogel on Self-Serve Analytics, R vs Pyt...              42   \n",
       "1     Machine Learning, Recommender Systems and the...              51   \n",
       "2     Defining the Data Problem, Data Science in Ma...              53   \n",
       "3     Data and Strategy, three Pillars of Research ...              60   \n",
       "4     Computer Forensics, Fraud Analytics and knowi...              63   \n",
       "..                                                 ...             ...   \n",
       "677   The A.I. and Machine Learning Landscape, with...              94   \n",
       "678   Automating Industrial Machines with Data Scie...              30   \n",
       "679                                            XGBoost              72   \n",
       "680         Business Intelligence Tools, with Mico Yuk              28   \n",
       "681   Contextual A.I. for Adapting to Adversaries, ...              81   \n",
       "\n",
       "                                   context_episode             guest_name  \\\n",
       "0                   Business Data Science Database           Ruben Kogel    \n",
       "1                    Machine Learning Data Science   Hadelin de Ponteves    \n",
       "2      Machine Learning R Programming Data Science        Dr. Wilson Pok    \n",
       "3                            Business Data Science         Brendan Hogan    \n",
       "4                                     Data Science        Dmitry Korneev    \n",
       "..                                             ...                    ...   \n",
       "677  Business Data Science Artificial Intelligence         George Mathew    \n",
       "678                          Business Data Science        Allegra Alessi    \n",
       "679           Machine Learning Data Science Python         Matt Harrison    \n",
       "680                          Business Data Science              Mico Yuk    \n",
       "681           Data Science Artificial Intelligence          Matar Haller    \n",
       "\n",
       "                                            guest_info  \\\n",
       "0    Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "1    Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "2    Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "3    Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "4    Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "..                                                 ...   \n",
       "677  Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "678  Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "679  Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "680  Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "681  Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "\n",
       "                                          text_episode  episode_number  \\\n",
       "0    Kirill: This is episode number one with ex-che...               1   \n",
       "1    Kirill: This is session number two with machin...               2   \n",
       "2    Kirill:\\tThis is episode number three with Nan...               3   \n",
       "3    Kirill: This is episode four with business str...               4   \n",
       "4    Kirill:\\tThis is episode number five with fore...               5   \n",
       "..                                                 ...             ...   \n",
       "677  Jon Krohn:\\t00:00:00This is episode number 679...             679   \n",
       "678  Jon Krohn:\\t00:06\\tThis is episode number 680 ...             680   \n",
       "679  Jon Krohn:\\t00:00:00\\tThis is episode number 6...             681   \n",
       "680  Jon Krohn:\\t00:05\\tThis is episode number 682 ...             682   \n",
       "681  Jon Krohn:\\t00:00:05\\tThis is episode number 6...             683   \n",
       "\n",
       "     episode_date  episode_year episode_day     host_episode  \n",
       "0    Sep 10, 2016          2016    Saturday  Kirill Eremenko  \n",
       "1    Sep 14, 2016          2016   Wednesday  Kirill Eremenko  \n",
       "2    Sep 25, 2016          2016      Sunday  Kirill Eremenko  \n",
       "3    Oct 02, 2016          2016      Sunday  Kirill Eremenko  \n",
       "4    Oct 09, 2016          2016      Sunday  Kirill Eremenko  \n",
       "..            ...           ...         ...              ...  \n",
       "677  May 16, 2023          2023     Tuesday        Jon Krohn  \n",
       "678  May 19, 2023          2023      Friday        Jon Krohn  \n",
       "679  May 23, 2023          2023     Tuesday        Jon Krohn  \n",
       "680  May 26, 2023          2023      Friday        Jon Krohn  \n",
       "681  May 30, 2023          2023     Tuesday        Jon Krohn  \n",
       "\n",
       "[682 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d810c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_pattern(row):\n",
    "    episode_split_text = []\n",
    "    speaker = []\n",
    "    episode_no = str(row['episode_number'])\n",
    "    guest_name = str(row['guest_name'].strip())\n",
    "    host_episode = str(row['host_episode'].strip())\n",
    "    text = str(row['text_episode'])\n",
    "\n",
    "    guest_first_name_1 = guest_name.split()[0]\n",
    "    guest_first_name_2 = guest_name.split()[1] if len(guest_name.split()) > 1 else '' # if any credentials are present in fron of name like Dr.\n",
    "    host_first_name = host_episode.split()[0]\n",
    "    guest_last_name = guest_name.strip().split()[-1]\n",
    "    guest_first_middle_name = ' '.join(guest_name.split()[:2])\n",
    "    guest_middle_last_name = ' '.join(guest_name.split()[1:3])\n",
    "    guest_first_last_name = \" \".join([guest_first_name_1, guest_last_name])\n",
    "    first_guest_name = ' '.join(guest_name.split()[:2])\n",
    "    second_guest_first_name = guest_name.strip().split()[-2] if len(guest_name.split()) > 1 else ''\n",
    "    second_guest_full_name = \" \".join([second_guest_first_name, guest_last_name]) if second_guest_first_name else ''\n",
    "   \n",
    "    pattern = f\"{guest_name}:|{host_episode}:|{guest_first_name_1}:|{guest_first_name_2}:|{host_first_name}:|{guest_last_name}:|{guest_first_middle_name}:|{guest_middle_last_name}:|{guest_first_last_name}:|{first_guest_name}:|{second_guest_full_name}:\"\n",
    "    # Find all occurrences of the pattern and their corresponding positions\n",
    "    pattern_matches = re.finditer(pattern, text)\n",
    "    \n",
    "    prev_end = 0\n",
    "    for match in pattern_matches:\n",
    "        start = match.start()\n",
    "        end = match.end()\n",
    "        \n",
    "        # Add the text between the previous and current pattern occurrence to the split_text list\n",
    "        episode_split_text.append(text[prev_end:start].strip())\n",
    "        \n",
    "        prev_end = end\n",
    "        \n",
    "        # Add the speaker information for the previous split text\n",
    "        speaker.append(text[start:end-1].strip())\n",
    "    \n",
    "    # Add the remaining text after the last pattern occurrence to the split_text list\n",
    "    episode_split_text.append(text[prev_end:].strip())\n",
    "    \n",
    "    # Add the speaker information for the remaining split text\n",
    "    speaker.append('')\n",
    "    \n",
    "    # Shift the index of the speaker list by 1\n",
    "    speaker = [''] + speaker[:-1]\n",
    "\n",
    "    # Create a DataFrame from the split text and speaker\n",
    "    text_df = pd.DataFrame({'episode_number': episode_no, 'episode_split_text': episode_split_text, 'speaker': speaker})\n",
    "    \n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84870029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s3/9_r9v9px7_v0q69b5zcbnmsm0000gn/T/ipykernel_21354/3066294325.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  split_text_df = split_text_df.append(split_text, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "split_text_df = pd.DataFrame(columns=['episode_number', 'episode_split_text', 'speaker'])\n",
    "for i, row in sds.iterrows():\n",
    "    split_text = find_pattern(row)\n",
    "    split_text_df = split_text_df.append(split_text, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60da34e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_number</th>\n",
       "      <th>episode_split_text</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This is episode number one with ex-chemical en...</td>\n",
       "      <td>Kirill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Hey guys, welcome to the Podcast. I’ve got Rub...</td>\n",
       "      <td>Kirill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Thank you! Thanks for having me over. I’m doin...</td>\n",
       "      <td>Ruben</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Awesome. It’s great to hear you and for those ...</td>\n",
       "      <td>Kirill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53011</th>\n",
       "      <td>683</td>\n",
       "      <td>01:17:22Yeah, right. Yeah, as I mean, it actua...</td>\n",
       "      <td>Jon Krohn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53012</th>\n",
       "      <td>683</td>\n",
       "      <td>01:17:45\\tHappy to.</td>\n",
       "      <td>Matar Haller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53013</th>\n",
       "      <td>683</td>\n",
       "      <td>01:17:46\\tNice. Well, yeah, so you mentioned p...</td>\n",
       "      <td>Jon Krohn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53014</th>\n",
       "      <td>683</td>\n",
       "      <td>01:18:04\\tThank you for having me. This was fa...</td>\n",
       "      <td>Matar Haller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53015</th>\n",
       "      <td>683</td>\n",
       "      <td>01:18:12\\tI loved this conversation today. I h...</td>\n",
       "      <td>Jon Krohn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53016 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode_number                                 episode_split_text  \\\n",
       "0                  1                                                      \n",
       "1                  1  This is episode number one with ex-chemical en...   \n",
       "2                  1  Hey guys, welcome to the Podcast. I’ve got Rub...   \n",
       "3                  1  Thank you! Thanks for having me over. I’m doin...   \n",
       "4                  1  Awesome. It’s great to hear you and for those ...   \n",
       "...              ...                                                ...   \n",
       "53011            683  01:17:22Yeah, right. Yeah, as I mean, it actua...   \n",
       "53012            683                                01:17:45\\tHappy to.   \n",
       "53013            683  01:17:46\\tNice. Well, yeah, so you mentioned p...   \n",
       "53014            683  01:18:04\\tThank you for having me. This was fa...   \n",
       "53015            683  01:18:12\\tI loved this conversation today. I h...   \n",
       "\n",
       "            speaker  \n",
       "0                    \n",
       "1            Kirill  \n",
       "2            Kirill  \n",
       "3             Ruben  \n",
       "4            Kirill  \n",
       "...             ...  \n",
       "53011     Jon Krohn  \n",
       "53012  Matar Haller  \n",
       "53013     Jon Krohn  \n",
       "53014  Matar Haller  \n",
       "53015     Jon Krohn  \n",
       "\n",
       "[53016 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d8976ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492    1\n",
       "300    1\n",
       "490    1\n",
       "132    1\n",
       "488    1\n",
       "486    1\n",
       "462    1\n",
       "460    1\n",
       "156    1\n",
       "186    1\n",
       "432    1\n",
       "431    1\n",
       "430    1\n",
       "182    1\n",
       "428    1\n",
       "426    1\n",
       "184    1\n",
       "424    1\n",
       "422    1\n",
       "158    1\n",
       "420    1\n",
       "190    1\n",
       "418    1\n",
       "192    1\n",
       "416    1\n",
       "414    1\n",
       "194    1\n",
       "410    1\n",
       "180    1\n",
       "434    1\n",
       "178    1\n",
       "436    1\n",
       "456    1\n",
       "160    1\n",
       "454    1\n",
       "162    1\n",
       "452    1\n",
       "164    1\n",
       "448    1\n",
       "166    1\n",
       "446    1\n",
       "168    1\n",
       "444    1\n",
       "170    1\n",
       "442    1\n",
       "440    1\n",
       "174    1\n",
       "438    1\n",
       "176    1\n",
       "130    1\n",
       "Name: episode_number, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text_df['episode_number'].value_counts().tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac30fa73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_number</th>\n",
       "      <th>episode_split_text</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10660</th>\n",
       "      <td>166</td>\n",
       "      <td>This is FiveMinuteFriday, and today we're talk...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode_number                                 episode_split_text  \\\n",
       "10660            166  This is FiveMinuteFriday, and today we're talk...   \n",
       "\n",
       "      speaker  \n",
       "10660          "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text_df[split_text_df['episode_number'] == '166']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f41d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern(row):\n",
    "    episode_split_text = []\n",
    "    speaker = []\n",
    "    episode_no = str(row['episode_number'])\n",
    "    guest_name = str(row['guest_name'].strip())\n",
    "    host_episode = str(row['host_episode'].strip())\n",
    "    text = str(row['text_episode'])\n",
    "\n",
    "    guest_first_name_1 = guest_name.split()[0]\n",
    "    #guest_first_name_2 = guest_name.split()[1] # if any credentials are present in fron of name like Dr.\n",
    "    host_first_name = host_episode.split()[0]\n",
    "    guest_last_name = guest_name.strip().split()[-1]\n",
    "    guest_first_middle_name = ' '.join(guest_name.split()[:2])\n",
    "    guest_middle_last_name = ' '.join(guest_name.split()[1:3])\n",
    "    guest_first_last_name = \" \".join([guest_first_name_1, guest_last_name])\n",
    "    first_guest_name = ' '.join(guest_name.split()[:2])\n",
    "    second_guest_first_name = guest_name.strip().split()[-2]\n",
    "    second_guest_full_name = \" \".join([second_guest_first_name, guest_last_name])\n",
    "   \n",
    "    pattern = f\"{guest_name}:|{host_episode}:|{guest_first_name_1}:|{guest_first_name_2}:|{host_first_name}:|{guest_last_name}:|{guest_first_middle_name}:|{guest_middle_last_name}:|{guest_first_last_name}:|{first_guest_name}:|{second_guest_full_name}:\"\n",
    "    # Find all occurrences of the pattern and their corresponding positions\n",
    "    pattern_matches = re.finditer(pattern, text)\n",
    "    \n",
    "    prev_end = 0\n",
    "    for match in pattern_matches:\n",
    "        start = match.start()\n",
    "        end = match.end()\n",
    "        \n",
    "        # Add the text between the previous and current pattern occurrence to the split_text list\n",
    "        episode_split_text.append(text[prev_end:start].strip())\n",
    "        \n",
    "        prev_end = end\n",
    "        \n",
    "        # Add the speaker information for the previous split text\n",
    "        speaker.append(text[start:end-1].strip())\n",
    "    \n",
    "    # Add the remaining text after the last pattern occurrence to the split_text list\n",
    "    episode_split_text.append(text[prev_end:].strip())\n",
    "    \n",
    "    # Add the speaker information for the remaining split text\n",
    "    speaker.append('')\n",
    "    \n",
    "    # Shift the index of the speaker list by 1\n",
    "    speaker = [''] + speaker[:-1]\n",
    "\n",
    "    # Create a DataFrame from the split text and speaker\n",
    "    text_df = pd.DataFrame({'episode_number': episode_no, 'episode_split_text': episode_split_text, 'speaker': speaker})\n",
    "    \n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42360a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147ca401",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text_df['episode_number'].value_counts().head(380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d1ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text_df[split_text_df['episode_number'] == '3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2579a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds['guest_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f7931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds.loc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text_df[split_text_df['episode_number'] == '591'].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds[sds['guest_name'] == ' Mars Buttfield-Addison ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds['text_episode'].loc[362]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c81ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe178b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the function to each row of the DataFrame sds\n",
    "split_text_df = sds.apply(find_pattern, axis=1).reset_index(drop=True)\n",
    "\n",
    "# Concatenate the individual DataFrames into a single DataFrame\n",
    "result_df = pd.concat(split_text_df.to_list(), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38fa04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def find_pattern(row):\n",
    "    episode_split_text = []\n",
    "    speaker = []\n",
    "    episode_no = row['episode_number']\n",
    "    guest_name = row['guest_name'].strip()\n",
    "    host_episode = row['host_episode'].strip()\n",
    "    text = row['text_episode']\n",
    "\n",
    "    guest_first_name = guest_name.split()[0]\n",
    "    host_first_name = host_episode.split()[0]\n",
    "    pattern = f\"{guest_name}:|{host_episode}:|{guest_first_name}:|{host_first_name}:\"\n",
    "\n",
    "    # Find all occurrences of the pattern and their corresponding positions\n",
    "    pattern_matches = re.finditer(pattern, text)\n",
    "    prev_end = 0\n",
    "    for match in pattern_matches:\n",
    "        start = match.start()\n",
    "        end = match.end()\n",
    "        \n",
    "        # Add the text between the previous and current pattern occurrence to the split_text list\n",
    "        episode_split_text.append(text[prev_end:start].strip())\n",
    "        \n",
    "        # Add the speaker information to the speaker list\n",
    "        speaker.append(text[start:end-1].strip())\n",
    "        \n",
    "        prev_end = end\n",
    "    \n",
    "    # Add the remaining text after the last pattern occurrence to the split_text list\n",
    "    episode_split_text.append(text[prev_end:].strip())\n",
    "    \n",
    "    # Add an additional entry in the speaker list for the remaining text\n",
    "    speaker.append('')\n",
    "\n",
    "    # Create a DataFrame from the split text and speaker\n",
    "    text_df = pd.DataFrame({'episode_split_text': episode_split_text, 'speaker': speaker})\n",
    "    \n",
    "    return text_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb61ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def find_pattern(row):\n",
    "    episode_split_text = []\n",
    "    speaker = []\n",
    "    episode_no = row['episode_number']\n",
    "    guest_name = row['guest_name'].strip()\n",
    "    host_episode = row['host_episode'].strip()\n",
    "    text = row['text_episode']\n",
    "\n",
    "    guest_first_name = guest_name.split()[0]\n",
    "    host_first_name = host_episode.split()[0]\n",
    "    pattern = f\"{guest_name}:|{host_episode}:|{guest_first_name}:|{host_first_name}:\"\n",
    "\n",
    "    # Find all occurrences of the pattern and their corresponding positions\n",
    "    pattern_matches = re.finditer(pattern, text)\n",
    "    prev_end = 0\n",
    "    for match in pattern_matches:\n",
    "        start = match.start()\n",
    "        end = match.end()\n",
    "        \n",
    "        # Add the text between the previous and current pattern occurrence to the split_text list\n",
    "        episode_split_text.append(text[prev_end:start].strip())\n",
    "        \n",
    "        prev_end = end\n",
    "        \n",
    "        # Add the speaker information for the previous split text\n",
    "        speaker.append(text[start:end-1].strip())\n",
    "    \n",
    "    # Add the remaining text after the last pattern occurrence to the split_text list\n",
    "    episode_split_text.append(text[prev_end:].strip())\n",
    "    \n",
    "    # Add the speaker information for the remaining split text\n",
    "    speaker.append('')\n",
    "\n",
    "    # Create a DataFrame from the split text and speaker\n",
    "    text_df = pd.DataFrame({'episode_split_text': episode_split_text, 'speaker': speaker})\n",
    "    \n",
    "    return text_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = sds.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0eb920",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_pattern(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern(row):\n",
    "    episode_no = row['episode_number']\n",
    "    guest_name = row['guest_name'].strip()\n",
    "    host_episode = row['host_episode'].strip()\n",
    "    text = row['text_episode']\n",
    "\n",
    "    guest_first_name = guest_name.split()[0]\n",
    "    host_first_name = host_episode.split()[0]\n",
    "    \n",
    "    pattern = f\"{guest_name}:|{host_episode}:|{guest_first_name}:|{host_first_name}:\"\n",
    "    \n",
    "    split_text = re.split(pattern, text)\n",
    "    speaker = re.findall(pattern,text)\n",
    "    text_df = pd.DataFrame(columns = {'episode_split_text': split_text , 'speaker' : speaker })\n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da071d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = sds.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4cec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88878887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313df70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_pattern(row):\n",
    "    guest_name = row['guest_name'].strip()\n",
    "    host_episode = row['host_episode'].strip()\n",
    "    text = row['text_episode']\n",
    "\n",
    "    guest_first_name = guest_name.split()[0]\n",
    "    host_first_name = host_episode.split()[0]\n",
    "    pattern = f\"{guest_name}:|{host_episode}:|{guest_first_name}:|{host_first_name}:\"\n",
    "    split_text = re.split(pattern, text)\n",
    "    \n",
    "    split_text_df = pd.DataFrame({'split_text': split_text})\n",
    "    return split_text_df\n",
    "\n",
    "# Apply the function to one row at a time and store the split text in a new DataFrame\n",
    "split_text_df = pd.DataFrame(columns=['split_text'])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    split_text = find_pattern(row)\n",
    "    split_text_df = split_text_df.append(split_text, ignore_index=True)\n",
    "\n",
    "print(split_text_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern(row):\n",
    "    guest_name = row['guest_name'].strip()\n",
    "    \n",
    "    host_episode = row['host_episode'].strip()\n",
    "    \n",
    "    text = row['text_episode']\n",
    "\n",
    "    guest_first_name = guest_name.split()[0]\n",
    "    host_first_name = host_episode.split()[0]\n",
    "    #print(host_first_name)\n",
    "    print(guest_first_name)\n",
    "    #pattern_1: f\"{guest_name}: {host_episode}:\"\n",
    "   # pattern_2: f\"{guest_first_name}: {host_first_name}:\" \n",
    "    pattern = f\"{guest_name}:|{host_episode}:|{guest_first_name}:|{host_first_name}:\"\n",
    "    print(pattern)\n",
    "    return re.split(pattern, text), re.findall(pattern,text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds.loc[681]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a87b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds['text_episode'].loc[681]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = sds.loc[681]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3705784",
   "metadata": {},
   "outputs": [],
   "source": [
    "text, speaker = find_pattern(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675a9926",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4760316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da697d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ac879",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812cc068",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_1.apply(find_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f8d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "row.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8242533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "row.apply(find_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5612a9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
