{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8fb172f",
   "metadata": {},
   "source": [
    "\n",
    "## SuperDataScience Podcast NLP Analysis\n",
    "\n",
    "### Notebook# 4: Text cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05fc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8056cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56dd6657",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb1ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds = pd.read_csv('../data/superdatascience.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "890ac8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 682 entries, 0 to 681\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   episode_name     682 non-null    object\n",
      " 1   length_episode   682 non-null    object\n",
      " 2   context_episode  682 non-null    object\n",
      " 3   guest_name       682 non-null    object\n",
      " 4   guest_info       682 non-null    object\n",
      " 5   text_episode     680 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 32.1+ KB\n"
     ]
    }
   ],
   "source": [
    "sds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4251eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_name</th>\n",
       "      <th>length_episode</th>\n",
       "      <th>context_episode</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>guest_info</th>\n",
       "      <th>text_episode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SDS 381: How to Avoid Failing at Digital Trans...</td>\n",
       "      <td>60 minutes</td>\n",
       "      <td>BusinessData Science</td>\n",
       "      <td>Podcast Guest: Tony SaldanhaWednesday Jul 08, ...</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Kirill Eremenko:\\tThis is episode number 381, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SDS 061: Discovering Data Science workflows an...</td>\n",
       "      <td>62 minutes</td>\n",
       "      <td>Machine LearningData SciencePython</td>\n",
       "      <td>Podcast Guest: Daniel WhitenackThursday Jun 15...</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>Kirill:\\tThis is episode number 61 with data s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        episode_name length_episode  \\\n",
       "0  SDS 381: How to Avoid Failing at Digital Trans...     60 minutes   \n",
       "1  SDS 061: Discovering Data Science workflows an...     62 minutes   \n",
       "\n",
       "                      context_episode  \\\n",
       "0                BusinessData Science   \n",
       "1  Machine LearningData SciencePython   \n",
       "\n",
       "                                          guest_name  \\\n",
       "0  Podcast Guest: Tony SaldanhaWednesday Jul 08, ...   \n",
       "1  Podcast Guest: Daniel WhitenackThursday Jun 15...   \n",
       "\n",
       "                                          guest_info  \\\n",
       "0  Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "1  Subscribe on Website, Apple Podcasts, Spotify,...   \n",
       "\n",
       "                                        text_episode  \n",
       "0  Kirill Eremenko:\\tThis is episode number 381, ...  \n",
       "1  Kirill:\\tThis is episode number 61 with data s...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sds.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39ce87eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_name</th>\n",
       "      <th>length_episode</th>\n",
       "      <th>context_episode</th>\n",
       "      <th>guest_name</th>\n",
       "      <th>guest_info</th>\n",
       "      <th>text_episode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>SDS 546: Daily Habit #4: Alternate-Nostril Bre...</td>\n",
       "      <td>5 minutes</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Podcast Guest: Jon KrohnThursday Feb 03, 2022</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>SDS 202: Ideas and Execution</td>\n",
       "      <td>7 minutes</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Podcast Guest: Kirill EremenkoFriday Oct 19, 2018</td>\n",
       "      <td>Subscribe on Website, Apple Podcasts, Spotify,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         episode_name length_episode  \\\n",
       "42  SDS 546: Daily Habit #4: Alternate-Nostril Bre...      5 minutes   \n",
       "66                       SDS 202: Ideas and Execution      7 minutes   \n",
       "\n",
       "   context_episode                                         guest_name  \\\n",
       "42    Data Science      Podcast Guest: Jon KrohnThursday Feb 03, 2022   \n",
       "66    Data Science  Podcast Guest: Kirill EremenkoFriday Oct 19, 2018   \n",
       "\n",
       "                                           guest_info text_episode  \n",
       "42  Subscribe on Website, Apple Podcasts, Spotify,...          NaN  \n",
       "66  Subscribe on Website, Apple Podcasts, Spotify,...          NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sds[sds[\"text_episode\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f504e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds = sds.dropna(subset=['text_episode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28de401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting episode_number and episode_name from episode_name column\n",
    "sds['episode_number'] = sds['episode_name'].str.split(':', expand = True)[0]\n",
    "sds['episode_name'] = sds['episode_name'].str.split(':', expand = True)[1]\n",
    "sds['episode_number'] = sds['episode_number'].str.replace('SDS', '')\n",
    "\n",
    "# Removing Podcast Guest text from guest_name column\n",
    "sds['guest_name'] = sds['guest_name'].str.split(':', expand = True)[1]\n",
    "\n",
    "# Removing minutes from length_episode column\n",
    "sds['length_episode'] = sds['length_episode'].str.split(' ', expand = True)[0]\n",
    "\n",
    "# Extracting date from guest_name column and making a new column episode_date\n",
    "sds['episode_date'] = sds['guest_name'].str.extract(r'(\\b[A-Za-z]{3}\\s\\d{2},\\s\\d{4}\\b)', expand = False).str.strip()\n",
    "\n",
    "# Converting episode_date into datetime object and changing format of date\n",
    "sds['episode_date'] = pd.to_datetime(sds['episode_date'], format='%b %d, %Y' )\n",
    "\n",
    "# Extracting year of episode\n",
    "sds['episode_year'] = sds['episode_date'].dt.year\n",
    "#sds['episode_year'] = sds['episode_date'].str.split(',', expand = True)[1]\n",
    "\n",
    "# Replacing/removing date in guest_name column \n",
    "sds['guest_name'] = sds['guest_name'].str.replace(r'(\\b[A-Za-z]{3}\\s\\d{2},\\s\\d{4}\\b)', '')\n",
    "\n",
    "# Extracting day from guest_name column and making a new column episode_day\n",
    "sds['episode_day'] = sds['guest_name'].str.extract(r'(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)', expand = False).str.strip()\n",
    "\n",
    "#Replacing/removing day in guest_name column \n",
    "sds['guest_name'] = sds['guest_name'].str.replace('Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday', '')\n",
    "\n",
    "#Adding spaces between some words in context_episode column\n",
    "sds['context_episode'] = sds['context_episode'].str.replace('([a-z])([A-Z])', r'\\1 \\2')\n",
    "\n",
    "# changing the data types for episode_number and length_episode\n",
    "sds['episode_number'] = sds['episode_number'].astype('int')\n",
    "sds['length_episode'] = sds['length_episode'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9243b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting dataframe on episode number\n",
    "sds = sds.sort_values('episode_number').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66c663fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column host_episode to sds dataframe\n",
    "sds['host_episode'] = 'Kirill Eremenko'  # Set initial value to 'Kirril Eremenko '\n",
    "\n",
    "# Set value to 'Jon Krohn' from column 430 to 681\n",
    "sds.loc[430:681, 'host_episode'] = 'Jon Krohn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bc10f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds.to_csv('../data/sds_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6c977",
   "metadata": {},
   "source": [
    "### Making separate dataframe for non datascience episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8722dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of non-datascience episodes\n",
    "topics = [' SEOI PEI – The skin of water listen now',' “What are you passionate about?”', ' Sweat every day',\n",
    "          ' Pride and humility',' Compartmentalization', ' Get out there!',' Failure', ' Happiness and problem solving',\n",
    "          ' Get in touch',' Secret to success', ' What is reality?',' Push yourself',' Teamwork', ' Getting things done', ' Selective ignorance',' Date with destiny', \n",
    "          ' Conferences',' Maker’s schedule vs. Manager’s schedule',  ' Dreams vs. Goals',' My best tool',\n",
    "          ' The best ideas', ' Stimulate your creativity',' The Quant Crunch',  ' Connecting the dots',' The five balls of life',  ' Do it for yourself',\n",
    "          ' Breaking patterns', ' Your questions', ' Why i became Vegetarian', ' Computer vision', ' Fermi questions',' Do what you want',  ' Exponential Thinking',  ' The power of now',' Willpower', \n",
    "          ' 100 Episodes',' Board Games',' Working Remotely',' AlphaGo Zero',  ' How to Win Friends & Influence People',' Expand Your Comfort Zone',\n",
    "          ' The Power of Gratitude', ' 1-on-1 with Kirill', ' New Year Resolutions',' Technological Singularity', \n",
    "          ' Who Moved My Cheese?',' Reckless Commitment', ' Ender’s Game', ' Instant Gratification Monkey',\n",
    "          ' How do Lobsters Grow?', ' Learn Blockchain!', ' Zone of Genius', ' Upper Limiting', ' Coworking',\n",
    "          ' Two Things to Remember and Two Things to Forget',' Empathy and Compassion', ' The Trolley Problem',\n",
    "          ' Have a Mentor',  ' Why Socializing is Vital', \" Parkinson's Law to increase Productivity\",\n",
    "          ' Maximize Life with the Rapid Planning Method',' Up Your Presentation Skills with Toastmasters',\n",
    "          ' What I Learned from a 10 Day Detox',' Love Languages and How They Impact Your Life',\n",
    "          ' Should You Be Effective or Efficient?',' Focus on Your Strengths and Ignore Your Weaknesses',\n",
    "          ' Tips for a Bumpy Ride', ' Why You Need to Go Beyond the Data',' A Technology Detox Challenge',\n",
    "          ' The Amazing Idea Behind Essentialism',  ' Why You Need Domain Knowledge in Data Science',\n",
    "          ' Why Execution Trumps Knowledge',' 6 Ways to Fill the Data Science Gap', \n",
    "          ' See You in San Diego',' Ace the “Greatest Weakness” Interview Question',' Why I Became Vegan', ' Two Millimeter Shifts',\n",
    "          ' What’s the Future of Data Science? Speaker Mashup Edition',' Ideas and Execution', \n",
    "          ' Set Your Goals Higher',' Re-live DSGO2018',' Compete With Yourself', ' What Is Amazing In Your Life', \n",
    "          ' Start A Great Day', ' 2018 in Numbers', ' Hacks for reading more books',\n",
    "          ' Flat Tyres Happen', ' Sleep on it', ' How to Deal with Negative Emotions', ' Meditation',\n",
    "          ' Guilt vs Shame', ' Two Wolves', ' Eating S.L.O.W.L.Y.',' You Cannot Make Progress Without a Routine',\n",
    "          ' Exploration vs Exploitation', ' The Cold is My Master',' The Six Months Rule',  ' Legacy', \n",
    "          ' Pura Vida',  ' Your Tribe', ' Make It About Yourself', ' Amazing',' Who Inspires You?', \n",
    "          ' Look for the Horse',' Good!',' Go through the Motions',' No Coaching', ' Better Than Perfect',\n",
    "          ' Too Many Photos', ' My Top 5 Productivity Hacks',' Coronavirus', ' Negative Coefficients', \n",
    "          ' Racism and Discrimination',' Importance of Sleep', ' Define Your Own Success',\n",
    "          ' Future-Proof Your Career', ' Remember to Wind Down',' Expose Yourself to New Ideas Regularly',\n",
    "          ' Use Your Unconscious Mind', ' Get a Headhunter', ' Perception vs. Emotion',' Start Your Own Morning Ritual', \n",
    "          ' Teach It', ' Five Job Hunting Tips', ' Emotional Burnout', ' Think Bigger',' Face Your Demons', \n",
    "          ' The Narrative Arc in Storytelling',' Abandon Hope', ' Meaning is Everything',' Communicate Your Needs', \n",
    "          ' Needs vs. Wants',' My Advice for Career Success',' Play With Feeling',' Wheel of Life', ' Pain vs. Suffering', ' The Shift',\n",
    "          ' The Internal Conflict Model', ' Intellect and Intelligence',' One-on-one with Kirill',\n",
    "          ' Hello from Jon and Welcome to 2021',' Attention Sharpening Tools Part 1',\n",
    "          ' Attention Sharpening Tools Part 2',' The Staggering Pace of Progress', ' The Pomodoro Technique',\n",
    "          ' Behind the Scenes', ' The History of Algebra',' It Could Be Even Better', \n",
    "          ' My Favorite Books',' The Learning Never Stops (so Relax)',' Peer-Driven Learning', ' Five Keys to Success', \n",
    "          ' Top Five Resume Tips',' The Continuous Calendar',' The History of Calculus',' The Price of Your Attention',\n",
    "          ' The World is Awful (and it’s Never Been Better)',' How to Instantly Appreciate Being Alive',\n",
    "          ' 2040',' How Only Beginners Know Everything',' Managing Imposter Syndrome', ' Building Your Ant Hill',\n",
    "          ' Does Caffeine Hurt Productivity? (Part 1)',' Does Caffeine Hurt Productivity? (Part 2',\n",
    "          ' Does Caffeine Hurt Productivity? (Part 3', ' Fail More',' The Normal Anxiety of Content Creation',\n",
    "          ' Mutable vs Immutable Conditions', ' A Holiday Greeting',' What I Learned in 2021', \n",
    "          ' Daily Habit #1',' Daily Habit #2',' Continuous Calendar for 2022', ' Daily Habit #3',\n",
    "          ' Daily Habit #4',' Daily Habit #5', ' Daily Habit #6',' The Most Popular SuperDataScience Episodes of 2021',\n",
    "          ' Daily Habit #7',' The Best Time to Plant a Tree',' Daily Habit #9', ' Music for Deep Work', \n",
    "          ' Tech Startup Dramas',' Daily Habit #10',' We Are Living in Ancient Times', ' Ignition',\n",
    "          ' Four Thousand Weeks',' Who Dares Wins',' More Guests on Fridays',' What I Learned in 2022',\n",
    "          ' The Most Popular SuperDataScience Podcast Episodes of 2022',\n",
    "          ' Career choice, disruptions in finance and application stacks',' How Constant Learning Created a Jet-Set Career',\n",
    "          ' Tips to Improve your Memory', ' How to be Happy and Successful',' Boost Your Self-Confidence',\n",
    "          ' Ask the Right Question', ' Your Core Strength', ' Learning Something New',' Solitude Deprivation',\n",
    "          ' Love Yourself',' The Passion Paradox', ' Who You Become',' Trial by Fire', ' Contemplation',\n",
    "          ' Meet the Team',' Mentorship',' Diets',' Proximity is Power #2',' Many Ways to Fail & Five Ways to Succeed in Startups',\n",
    "          ' The Power of Coaching',' Emotions, Relationships, and Being Kind During the Pandemic',\n",
    "          ' Depression and Suicidal Thoughts',' The Power of Women in STEM', ' Maelstrom, Chaos, and Mayhem',\n",
    "          ' 10 Tips to Become a Master Presenter', ' Stand More - Sit Less', ' Real-World Applications of Digital Twins',\n",
    "          ' The End of Jobs',' Yoga Nidra', ' Open-Access Publishing',' Yoga Nidra Practice with Steve Fazzari',\n",
    "          ' Daily Habit #11',' Thriving on Information Overload',' The Four Requirements for Expertise (beyond the “10,000 Hours”)',\n",
    "          ' The Joy of Atelic Activities',' Burnout']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0bccbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73e0132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create dataframe with non datascience episodes\n",
    "def subset_by_episode_name(df, episode_names):\n",
    "    return df[df['episode_name'].isin(episode_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e018af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_datascience = subset_by_episode_name(sds, topics).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e172585",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_datascience['class'] = ''\n",
    "non_datascience['class'] = non_datascience['class'].replace('', 'Health, Life and Philosophy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25a06c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_datascience.to_csv('../data/non_datascience.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ac83a1",
   "metadata": {},
   "source": [
    "## Creating data science episode dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba18c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to leave datascience episodes in sds dataframe\n",
    "def remove_by_episode_name(df, episode_names):\n",
    "    return df[~df['episode_name'].isin(episode_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78800ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_datascience = remove_by_episode_name(sds, topics).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae5caa",
   "metadata": {},
   "source": [
    "### Dividing episodes into proper categories/ recategorizing episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea4010ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sds_datascience['context_episode'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aae71ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicating context_episode column\n",
    "sds_datascience['class'] = sds_datascience['context_episode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eef7afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Visualization = ['Data Science Database Data Visualization',\n",
    "                      'Tableau Data Visualization',\n",
    "                      'Business Data Visualization',\n",
    "                      'Data Science Data Visualization',\n",
    "                      'Data Science Tableau Excel Data Visualization',\n",
    "                      'Data Science Tableau Data Visualization',\n",
    "                      'Tableau Database Data Visualization',\n",
    "                      'Business Data Science Tableau Data Visualization',\n",
    "                      'Business Tableau Data Visualization',\n",
    "                      'Data Science Excel Data Visualization',\n",
    "                      'Business Data Science Data Visualization']\n",
    "\n",
    "sds_datascience['class'] = sds_datascience['class'].replace(Data_Visualization, 'Data Visualization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3cc127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Programming = ['Excel', \n",
    "               'Python',\n",
    "               'Business Data Science Excel',\n",
    "               'R Programming Data Science Python',\n",
    "               'Business R Programming',\n",
    "               'R Programming Python', \n",
    "               'Business R Programming Data Science',\n",
    "               'R Programming Database', \n",
    "               'Data Science Python',\n",
    "               'R Programming Data Science',\n",
    "               'R Programming Data Science Excel']\n",
    "\n",
    "sds_datascience['class'] = sds_datascience['class'].replace(Programming, 'Programming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5dc67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Programming_Data_Visualization = ['Business R Programming Tableau Data Visualization',\n",
    "                             'R Programming Data Science Data Visualization',\n",
    "                             'R Programming Tableau Database Data Visualization',\n",
    "                             'Business R Programming Data Science Python Database Data Visualization']\n",
    "\n",
    "sds_datascience['class'] = sds_datascience['class'].replace(Programming_Data_Visualization, 'Programming and Data Visualization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "096da746",
   "metadata": {},
   "outputs": [],
   "source": [
    "Database = ['Business Data Science Database', \n",
    "            'Data Science Python Database',\n",
    "            'Business Python Database',\n",
    "            'Database',\n",
    "            'Business Database', \n",
    "            'Data Science Database']\n",
    "\n",
    "sds_datascience['class'] = sds_datascience['class'].replace(Database, 'Database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa968091",
   "metadata": {},
   "outputs": [],
   "source": [
    "Machine_Learning = ['Machine Learning Data Science',\n",
    "                     'Machine Learning R Programming Data Science',\n",
    "                     'Machine Learning', \n",
    "                     'Business Machine Learning R Programming Data Science Python Excel',\n",
    "                     'Machine Learning R Programming Data Science Excel Database',\n",
    "                     'Business Machine Learning Python Database',\n",
    "                     'Machine Learning Data Science Python',\n",
    "                     'Business Machine Learning Data Science Python',\n",
    "                     'Business Machine Learning Tableau Data Visualization',\n",
    "                     'Business Machine Learning Data Science', \n",
    "                     'Machine Learning Data Science Database',\n",
    "                     'Business Machine Learning', \n",
    "                     'Machine Learning Python Database',\n",
    "                     'Machine Learning Data Visualization',\n",
    "                     'Machine Learning Data Science Productivity',\n",
    "                     'Business Machine Learning R Programming Data Science']\n",
    "\n",
    "sds_datascience['class'] = sds_datascience['class'].replace(Machine_Learning, 'Machine Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4165146",
   "metadata": {},
   "outputs": [],
   "source": [
    "Machine_Learning_AI = ['Business Machine Learning Data Science Artificial Intelligence',\n",
    "                        'Machine Learning Data Science Artificial Intelligence Blockchain',\n",
    "                        'Business Machine Learning Data Science Artificial Intelligence Database',\n",
    "                        'Machine Learning Data Science Artificial Intelligence']\n",
    "\n",
    "sds_datascience['class'] = sds_datascience['class'].replace(Machine_Learning_AI, 'Machine Learning and Artificial Intelligence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01717ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI = ['Artificial Intelligence', \n",
    "        'Business Artificial Intelligence',\n",
    "        'Business Data Science Artificial Intelligence',\n",
    "        'Business Artificial Intelligence Database',\n",
    "        'Data Science Artificial Intelligence']\n",
    "\n",
    "sds_datascience['class'] = sds_datascience['class'].replace(AI, 'Artificial Intelligence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "170fbd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "Machine_Learning_Deep_Learning = ['Machine Learning Data Science Python Deep Learning',\n",
    "                                    'Business Machine Learning Data Science Deep Learning',\n",
    "                                    'Machine Learning Data Science Deep Learning',\n",
    "                                    'Business Machine Learning R Programming Data Science Python Deep Learning',\n",
    "                                    'Machine Learning Data Science Database Deep Learning',\n",
    "                                    'Data Science Deep Learning']\n",
    "\n",
    "sds_datascience['class'] = sds_datascience['class'].replace(Machine_Learning_Deep_Learning, 'Machine Learning and Deep Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0359df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Machine_Learning_AI_Deep_Learning = ['Business Machine Learning Data Science Python Artificial Intelligence Deep Learning',\n",
    "                 'Business Machine Learning Data Science Artificial Intelligence Deep Learning',\n",
    "                 'Machine Learning Data Science Python Artificial Intelligence Database Deep Learning',\n",
    "                 'Machine Learning Data Science Artificial Intelligence Deep Learning',\n",
    "                 'Machine Learning Data Science Artificial Intelligence Database Blockchain Deep Learning',\n",
    "                 'Business Machine Learning R Programming Data Science Artificial Intelligence Deep Learning',\n",
    "                 'Machine Learning Artificial Intelligence Blockchain Deep Learning',\n",
    "                 'Machine Learning Data Science Artificial Intelligence Blockchain Deep Learning',\n",
    "                 'Machine Learning Artificial Intelligence Deep Learning']\n",
    "\n",
    "sds_datascience['class'] = sds_datascience['class'].replace(Machine_Learning_AI_Deep_Learning, 'Machine Learning, Artificial Intelligence and Deep Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "811564c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_Deep_Learning = ['Data Science Artificial Intelligence Deep Learning',\n",
    "                       'Business Data Science Artificial Intelligence Deep Learning']\n",
    "\n",
    "sds_datascience['class'] = sds_datascience['class'].replace(AI_Deep_Learning, 'Artificial Intelligence and Deep Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f07554aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Science = ['Business Data Science', \n",
    "                  'Data Science', \n",
    "                  'Uncategorized',\n",
    "                  'Business', \n",
    "                  'Data Science Statistics', \n",
    "                ' Data Science in Retail',\n",
    "                  'Life Philosophy Data Science']\n",
    "\n",
    "sds_datascience['class'] = sds_datascience['class'].replace(Data_Science, 'Data Science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "550c57d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Blockchain = ['Data Science Blockchain',\n",
    "              'Database Blockchain',\n",
    "              'Blockchain']\n",
    "\n",
    "sds_datascience['class'] = sds_datascience['class'].replace(Blockchain, 'Blockchain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46b9239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Science_Productivity = ['Data Science Python Productivity',\n",
    "                            'Data Science Productivity']\n",
    "\n",
    "sds_datascience['class'] = sds_datascience['class'].replace(Data_Science_Productivity, 'Data Science and Productivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7819ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Deep_Learning = ['Data Science Deep Learning', \n",
    "                 'Business Data Science Deep Learning']\n",
    "\n",
    "sds_datascience['class'] = sds_datascience['class'].replace(Deep_Learning, 'Deep Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a00c9632",
   "metadata": {},
   "outputs": [],
   "source": [
    "career = ['Data Science Career Tips',\n",
    "          'Business Data Science Career Tips',\n",
    "          'Data Science Python Career Tips',\n",
    "          'Business Data Science Artificial Intelligence Career Tips',\n",
    "          'Machine Learning Data Science Career Tips']\n",
    "sds_datascience['class'] = sds_datascience['class'].replace(career, 'Data Science Career')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06a249b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_datascience.to_csv('../data/sds_datascience.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d117e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to dialouges of host and guest for each episode and store in separate rows\n",
    "def find_pattern(row):\n",
    "    episode_split_text = []\n",
    "    speaker = []\n",
    "    episode_no = str(row['episode_number'])\n",
    "    guest_name = str(row['guest_name'].strip())\n",
    "    host_episode = str(row['host_episode'].strip())\n",
    "    text = str(row['text_episode'])\n",
    "\n",
    "    # Different patterns of guest name and host name present in text\n",
    "    guest_first_name_1 = guest_name.split()[0]\n",
    "    guest_first_name_2 = guest_name.split()[1] if len(guest_name.split()) > 1 else '' # if any credentials are present in fron of name like Dr.\n",
    "    host_first_name = host_episode.split()[0]\n",
    "    guest_last_name = guest_name.strip().split()[-1]\n",
    "    guest_first_middle_name = ' '.join(guest_name.split()[:2])\n",
    "    guest_middle_last_name = ' '.join(guest_name.split()[1:3])\n",
    "    guest_first_last_name = \" \".join([guest_first_name_1, guest_last_name])\n",
    "    first_guest_name = ' '.join(guest_name.split()[:2])\n",
    "    second_guest_first_name = guest_name.strip().split()[-2] if len(guest_name.split()) > 1 else ''\n",
    "    second_guest_full_name = \" \".join([second_guest_first_name, guest_last_name]) if second_guest_first_name else ''\n",
    "   \n",
    "    # Conditions to look in text\n",
    "    pattern = f\"{guest_name}:|{host_episode}:|{guest_first_name_1}:|{guest_first_name_2}:|{host_first_name}:|{guest_last_name}:|{guest_first_middle_name}:|{guest_middle_last_name}:|{guest_first_last_name}:|{first_guest_name}:|{second_guest_full_name}:\"\n",
    "    \n",
    "    # Find all occurrences of the pattern and their corresponding positions\n",
    "    pattern_matches = re.finditer(pattern, text)\n",
    "    \n",
    "    prev_end = 0\n",
    "    for match in pattern_matches:\n",
    "        start = match.start()\n",
    "        end = match.end()\n",
    "        \n",
    "        # Add the text between the previous and current pattern occurrence to the split_text list\n",
    "        episode_split_text.append(text[prev_end:start].strip())\n",
    "        \n",
    "        prev_end = end\n",
    "        \n",
    "        # Add the speaker information for the previous split text\n",
    "        speaker.append(text[start:end-1].strip())\n",
    "    \n",
    "    # Add the remaining text after the last pattern occurrence to the split_text list\n",
    "    episode_split_text.append(text[prev_end:].strip())\n",
    "    \n",
    "    # Add the speaker information for the remaining split text\n",
    "    speaker.append('')\n",
    "    \n",
    "    # Shift the index of the speaker list by 1\n",
    "    speaker = [''] + speaker[:-1]\n",
    "\n",
    "    # Create a DataFrame from the split text and speaker\n",
    "    text_df = pd.DataFrame({'episode_number': episode_no, 'episode_split_text': episode_split_text, 'speaker': speaker})\n",
    "    \n",
    "    return text_df\n",
    "\n",
    "\n",
    "# Loop to process each row of sds_datascience dataframe\n",
    "sds_ds_split = pd.DataFrame(columns=['episode_number', 'episode_split_text', 'speaker'])\n",
    "for i, row in sds_datascience.iterrows():\n",
    "    split_text = find_pattern(row)\n",
    "    sds_ds_split = sds_ds_split.append(split_text, ignore_index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_split['episode_split_text'].apply(lambda x: isinstance(x, str) and len(x.strip()) == 0).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe984106",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_text = sds_ds_split[sds_ds_split['episode_split_text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_text['episode_number'] = sds_ds_text['episode_number'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_text = pd.merge(sds_datascience, sds_ds_text, on = 'episode_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace('\\\\t|\\\\n|\\\\xa0', '')\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace('\\(?\\d{2}:\\d{2}\\):| ?(\\d{2}:)+\\d{2}', '')\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace('\\(background music plays\\)', '')\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace('\\(background music\\)', '')\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace('\\[inaudible\\]', '')\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace('\\(inaudible\\)', '')\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"That\\\\'s\", ' That is')\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"it\\\\'s\", \"it is\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"They\\\\'re\", \" They are\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"they\\\\'re\", \"they are\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"that\\\\'s\", \"that is\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"you\\\\'re\", \"you are\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"You\\\\'re\", \" You are\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"I\\\\'m\", \" I am\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"here\\\\'s\", \"here is\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"You\\'re\", \"You are\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"you\\'re\", \"you are\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"we\\'re\", \"we are\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"I\\'ve\", \"I have\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"I\\'ll\", \"I will\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"He\\'s\", \"He is\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"he\\'s\", \"he is\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"what\\'s\", \"what is\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"here\\\\'s\", \"here is\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"there\\\\'s\", \"there is\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"isn\\\\'t\", \"is not\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"didn\\\\'t\", \"did not\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"Yeah, yeah, yeah, yeah\", \" Yes\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"Yeah\", \" Yes\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"yeah\", \"yes\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"don\\\\'t\", \"do not\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"’\", \"'\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"haven\\'t\", \"have not\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"doesn\\'t\", \"does not\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"we\\'ve\", \"we have\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"we\\'d\", \"we had\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"It\\'s\", \" It is\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"can\\'t\", \"cannot\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"who\\'s\", \"whose\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"wouldn\\'t\", \"would not\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"there\\'re\", \"there are\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"There\\'re\", \" There are\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"it\\'s\", \"it is\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"What\\'s\", \" What is\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"Right, right, right\", \"Right\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"you\\'ll\", \"you will\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"there\\'ll\", \"there will\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"let\\'s\", \"let us\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"Yes, yes, yes\", \"yes\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"would\\'ve\", \"would have\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"AI\", \"artificial intelligence\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"what\\\\'ll\", \"what will\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"won\\'t\", \"would not\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"shouldn\\'t\", \"should not\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"\\[crosstalk\\]\", \"\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"A.I.\", \"artificial intelligence\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"Five-Minute\", \"Five Minute\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"SuperDataScience\", \"\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"I\\'d\", \"I had\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"They\\'ve\", \"They Have\")\n",
    "sds_ds_text['episode_split_text'] = sds_ds_text['episode_split_text'].str.replace(\"I\\'m\", \"I am\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67929f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_ds_text.to_csv('../data/sds_ds_text.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79321d7",
   "metadata": {},
   "source": [
    "### Processing non datascience episode dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c04d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to process each row of non_datascience dataframe\n",
    "sds_nds_split = pd.DataFrame(columns=['episode_number', 'episode_split_text', 'speaker'])\n",
    "for i, row in non_datascience.iterrows():\n",
    "    split_text = find_pattern(row)\n",
    "    sds_nds_split = sds_nds_split.append(split_text, ignore_index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c24660",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_split['episode_split_text'].apply(lambda x: isinstance(x, str) and len(x.strip()) == 0).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540770e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_split['episode_number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_text = sds_nds_split[sds_nds_split['episode_split_text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67591c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_text['episode_number'] = sds_nds_text['episode_number'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d582175",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_text = pd.merge(non_datascience, sds_nds_text, on = 'episode_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a405ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_text['episode_split_text'] = sds_nds_text['episode_split_text'].str.replace('\\\\t|\\\\n|\\\\xa0', '')\n",
    "sds_nds_text['episode_split_text'] = sds_nds_text['episode_split_text'].str.replace('\\(?\\d{2}:\\d{2}\\):| ?(\\d{2}:)+\\d{2}', '')\n",
    "sds_nds_text['episode_split_text'] = sds_nds_text['episode_split_text'].str.replace('\\(background music plays\\)', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_nds_text.to_csv('../data/sds_nds_text.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889dce23",
   "metadata": {},
   "source": [
    "### function used before to split columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1397c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the text over the rows\n",
    "\n",
    "def text_split(df, column):\n",
    "    \n",
    "    sds_index = column.str.findall('(?:^|\\\\xa0|(?:\\\\n)+)([A-Za-z\\s]+):').explode().to_frame()\n",
    "    sds_index['episode_index'] = 1 \n",
    "    sds_index['episode_index'] = sds_index.groupby(sds_index.index)['episode_index'].cumsum()\n",
    "    sds_index = sds_index.reset_index().set_index(['index','episode_index']).rename(columns = {column.name :'speaker'})\n",
    "    #sds_index\n",
    "    split_text = column.str.split('(?:^|\\\\xa0|(?:\\\\n)+)[A-Za-z\\s]+:', expand = True).stack().to_frame()#.droplevel(-1)\n",
    "    \n",
    "    split_text.index = split_text.index.rename(sds_index.index.names)\n",
    "    #return sds_index,split_text \n",
    "    sds_index = sds_index.merge(split_text, left_index = True, right_index=True, how = 'right').droplevel(-1)\n",
    "    return df.merge(sds_index,left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9717973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call function text_split\n",
    "sds_text = text_split(sds, sds['text_episode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b72bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_text = sds_text.rename(columns = {0 : 'episode_split_text' })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
